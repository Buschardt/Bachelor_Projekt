{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as mpl\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import random, numpy as np, pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "import scipy.cluster\n",
    "import math\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Equally Weighted Portfolio\n",
    "def ewPortfolio(cov,**kargs):\n",
    "    n=len(cov)\n",
    "    return n*[1/n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Risk Parity\n",
    "def calculate_risk_contribution(w,V):\n",
    "    # function that calculates asset contribution to total risk\n",
    "    w = np.matrix(w)\n",
    "    sigma = np.sqrt(calculate_portfolio_var(w,V))\n",
    "    # Marginal Risk Contribution\n",
    "    MRC = np.dot(V,w.T)\n",
    "    # Risk Contribution\n",
    "    RC = np.multiply(MRC,w.T)/sigma\n",
    "    return RC\n",
    "\n",
    "def risk_budget_objective(x,pars):\n",
    "    # calculate portfolio risk\n",
    "    V = pars[0]# covariance table\n",
    "    x_t = pars[1] # risk target in percent of portfolio risk\n",
    "    sig_p =  np.sqrt(calculate_portfolio_var(x,V)) # portfolio sigma\n",
    "    risk_target = np.asmatrix(np.multiply(sig_p,x_t))\n",
    "    asset_RC = calculate_risk_contribution(x,V)\n",
    "    J = sum(np.square(asset_RC-risk_target.T)) # sum of squared error\n",
    "    return J\n",
    "\n",
    "def risk_parity(cov,**kargs):\n",
    "    riskbudget=np.array([1/len(cov)]*len(cov))\n",
    "    w0=np.array([1/len(cov)]*len(cov))\n",
    "    x_t = riskbudget # your risk budget percent of total portfolio risk (equal risk)\n",
    "    cons = ({'type': 'eq', 'fun': total_weight_constraint},\n",
    "    {'type': 'ineq', 'fun': long_only_constraint})\n",
    "    res= minimize(risk_budget_objective, w0, args=[cov,x_t],tol=0.000000000000001,\n",
    "                  method='SLSQP',constraints=cons) \n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global minimum variance - long only\n",
    "def GMVLOPortfolio(cov,**kargs):\n",
    "    x0=pd.Series([1/len(cov)]*len(cov))\n",
    "    cons = ({'type': 'eq', 'fun': total_weight_constraint},\n",
    "    {'type': 'ineq', 'fun': long_only_constraint})\n",
    "    res = minimize(calculate_portfolio_var,x0,args=cov,tol=0.000000000000001,\n",
    "                   method='SLSQP',constraints=cons)\n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse variance portfolio (Risk parity fra De Prado)\n",
    "def getIVP(cov,**kargs):\n",
    "    #Compute the inverse-variance portfolio\n",
    "    ivp=1/np.diag(cov)\n",
    "    ivp/=ivp.sum()\n",
    "    return ivp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximum diversification\n",
    "def calculate_portfolio_var(w,V):\n",
    "    # function that calculates portfolio risk\n",
    "    return (np.dot(np.dot(w,V),w.T))\n",
    "\n",
    "def calc_diversification_ratio(w, V):\n",
    "    # average weighted vol\n",
    "    w_vol = np.dot(np.sqrt(np.diag(V)), w.T)\n",
    "    # portfolio vol\n",
    "    port_vol = np.sqrt(calculate_portfolio_var(w, V))\n",
    "    diversification_ratio = w_vol/port_vol\n",
    "    # return negative for minimization problem (maximize = minimize -)\n",
    "    return -diversification_ratio\n",
    "\n",
    "def total_weight_constraint(x):\n",
    "    return np.sum(x)-1\n",
    "\n",
    "def long_only_constraint(x):\n",
    "    return x\n",
    "\n",
    "def max_div_port(cov,**kargs):\n",
    "    # w0: initial weight\n",
    "    # V: covariance matrix\n",
    "    # bnd: individual position limit\n",
    "    # long only: long only constraint\n",
    "    bnd=None\n",
    "    long_only=True\n",
    "    w0=np.array([1/len(cov)]*len(cov))\n",
    "    cons = ({'type': 'eq', 'fun': total_weight_constraint},)\n",
    "    if long_only: # add in long only constraint\n",
    "        cons = cons + ({'type': 'ineq', 'fun':  long_only_constraint},)\n",
    "    res = minimize(calc_diversification_ratio, w0, bounds=bnd,\n",
    "                   args=cov, method='SLSQP', constraints=cons)\n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HRP2\n",
    "#This version of HRP divides the weights between clusters\n",
    "\n",
    "import collections\n",
    "def flatten(x):\n",
    "    if isinstance(x, collections.Iterable):\n",
    "        return [a for i in x for a in flatten(i)]\n",
    "    else:\n",
    "        return [x]\n",
    "    \n",
    "def get_cluster_dict(link):\n",
    "    link=np.append(link,np.array([[j] for j in range(int(link[-1,3]),\n",
    "                                                     int(link[-1,3])*2-1)]),axis=1)\n",
    "    cluster_dict={}\n",
    "    for i in link[:,0:5].astype(int):\n",
    "        cluster_dict[i[4]]=[]\n",
    "        if i[0]>=link[0,-1]:\n",
    "            cluster_dict[i[4]].append(cluster_dict[i[0]])\n",
    "        else:\n",
    "            cluster_dict[i[4]].append(i[0])\n",
    "        if i[1]>=link[0,-1]:\n",
    "            cluster_dict[i[4]].append(cluster_dict[i[1]])\n",
    "        else:\n",
    "            cluster_dict[i[4]].append(i[1])\n",
    "        \n",
    "    return cluster_dict\n",
    "\n",
    "\n",
    "def recClusterVar(cluster_dict,link, cov):\n",
    "    link=np.append(link,np.array([[j] for j in range(int(link[-1,3]),\n",
    "                                                     int(link[-1,3])*2-1)]),axis=1)\n",
    "    w=pd.Series(1,index=[i for i in range(int(link[0,-1]))]) \n",
    "    for i in reversed(link.astype(int)):\n",
    "        if i[0]>=link[0,-1]:\n",
    "            cluster1 = cluster_dict[i[0]]\n",
    "        else:\n",
    "            cluster1 = i[0]\n",
    "\n",
    "        if i[1]>=link[0,-1]:\n",
    "            cluster2 = cluster_dict[i[1]]\n",
    "        else:\n",
    "            cluster2 = i[1]\n",
    "        cluster1=[i for i in flatten(cluster1)]\n",
    "        cluster2=[i for i in flatten(cluster2)]\n",
    "        c1_var=getClusterVar(cov,cluster1)\n",
    "        c2_var=getClusterVar(cov,cluster2)\n",
    "        alpha=1-c1_var/(c1_var+c2_var)\n",
    "        w[cluster1]*=alpha # weight 1\n",
    "        w[cluster2]*=1-alpha # weight 2\n",
    "        #print(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HRP\n",
    "def getClusterVar(cov,cItems):\n",
    "    #Compute variance per cluster\n",
    "    cov_=cov.loc[cItems,cItems] # matrix slice\n",
    "    w_=getIVP(cov_).reshape(-1,1)\n",
    "    cVar=np.dot(np.dot(w_.T,cov_),w_)[0,0]\n",
    "    return cVar\n",
    "\n",
    "def getQuasiDiag(link):\n",
    "    # Sort clustered items by distance\n",
    "    link=link.astype(int)\n",
    "    sortIx=pd.Series([link[-1,0],link[-1,1]])\n",
    "    numItems=link[-1,3] #number of original items\n",
    "    while sortIx.max()>=numItems:\n",
    "        sortIx.index=range(0,sortIx.shape[0]*2,2) #make space\n",
    "        df0=sortIx[sortIx>=numItems] # find clusters\n",
    "        i = df0.index;j=df0.values-numItems\n",
    "        sortIx[i]=link[j,0] # item 1\n",
    "        df0=pd.Series(link[j,1],index=i+1)\n",
    "        sortIx=sortIx.append(df0) # item 2\n",
    "        sortIx=sortIx.sort_index() #re-sort\n",
    "        sortIx.index=range(sortIx.shape[0]) # re-index\n",
    "    return sortIx.tolist()\n",
    "\n",
    "def getRecBipart(cov,sortIx):\n",
    "    # Compute HRP alloc\n",
    "    w=pd.Series(1,index=sortIx)\n",
    "    cItems=[sortIx] # initialize all items in one cluster\n",
    "    while len(cItems)>0:\n",
    "        cItems=[i[j:k] for i in cItems for j,\n",
    "                k in ((0,len(i)//2),(len(i)//2,len(i))) if len(i)>1] # bi-section\n",
    "        \n",
    "        for i in range(0,len(cItems),2):\n",
    "            cItems0=cItems[i] # cluster 1\n",
    "            cItems1=cItems[i+1] # cluster 2\n",
    "            cVar0=getClusterVar(cov,cItems0)\n",
    "            cVar1=getClusterVar(cov,cItems1)\n",
    "            alpha=1-cVar0/(cVar0+cVar1)\n",
    "            w[cItems0]*=alpha # weight 1\n",
    "            w[cItems1]*=1-alpha # weight 2\n",
    "            \n",
    "    w.sort_index(inplace=True)\n",
    "    return w\n",
    "    \n",
    "\n",
    "def correlDist(corr):\n",
    "    # A distance matrix based on correlation, where 0<=d[i,j]<=1\n",
    "    #This is a proper diastance metric\n",
    "    dist=((1-corr)/2.)**.5 # distance matrix\n",
    "    return dist\n",
    "\n",
    "\n",
    "def plotCorrMatrix(path,corr,labels=None):\n",
    "    #Heatmap of the correlation matrix\n",
    "    if labels is None: labels=[]\n",
    "    mpl.pcolor(corr)\n",
    "    mpl.colorbar()\n",
    "    mpl.yticks(np.arange(.5,corr.shape[0]+.5),labels)\n",
    "    mpl.xticks(np.arange(.5,corr.shape[0]+.5),labels)\n",
    "    mpl.savefig(path)\n",
    "    mpl.clf();mpl.close()  #reset pylab\n",
    "    return\n",
    "\n",
    "    \n",
    "def findCorrelatedCols(colnbs,size0):\n",
    "    keys = list(set([i[0] for i in colnbs]))\n",
    "    for i in range(1,size0+1):\n",
    "        if i not in keys:\n",
    "            keys.append(i)\n",
    "    keys.sort()\n",
    "    clusters={key: [key] for key in keys}\n",
    "    for i in colnbs:\n",
    "        clusters[i[0]].append(i[1])\n",
    "    return clusters\n",
    "\n",
    "def clusterWeights(clusters, hrp):\n",
    "    weights={key:None for key in clusters.keys()}\n",
    "    for i in weights:\n",
    "        weights[i] = sum([hrp.loc[j] for j in clusters[i]])\n",
    "    return list(weights.values())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Barplots of allocations\n",
    "def plot_weights(hrp, hrp2, cov, data):\n",
    "    index = list(data.columns)\n",
    "    \n",
    "    #HRP\n",
    "    hrp = hrp.sort_values(ascending=False)\n",
    "    hrp.plot.bar(figsize = (15,7))\n",
    "    mpl.title(\"HRP\")\n",
    "    mpl.ylabel(\"Weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((0, 0.5))\n",
    "    mpl.show()\n",
    "    \n",
    "    #HRP2\n",
    "    hrp2 = hrp2.sort_values(ascending=False)\n",
    "    hrp2.plot.bar(figsize = (15,7))\n",
    "    mpl.title(\"HRP2\")\n",
    "    mpl.ylabel(\"Weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((0, 0.5))\n",
    "    mpl.show()\n",
    "\n",
    "    #Naive Risk-Parity\n",
    "    ivp = getIVP(cov)\n",
    "    ivp = pd.Series(ivp, index=index)\n",
    "    ivp = ivp.sort_values(ascending=False)\n",
    "    ivp.plot.bar(figsize = (15,7))\n",
    "    mpl.title(\"Naive Risk parity (IVP)\")\n",
    "    mpl.ylabel(\"Weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((0,0.5))\n",
    "    mpl.show()\n",
    "    \n",
    "    #Risk Parity\n",
    "    rp = risk_parity(cov)\n",
    "    rp = pd.Series(rp, index=index)\n",
    "    rp = rp.sort_values(ascending=False)\n",
    "    rp.plot.bar(figsize = (15,7))\n",
    "    mpl.title(\"Risk Parity\")\n",
    "    mpl.ylabel(\"Weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((0, 0.5))\n",
    "    mpl.show()\n",
    "    \n",
    "    #GMV\n",
    "    gmv = GMVPortfolio(cov)\n",
    "    gmv = pd.Series(gmv, index=index)\n",
    "    gmv = gmv.sort_values(ascending=False)\n",
    "    gmv.plot.bar(figsize = (15,7))\n",
    "    mpl.title(\"GMV\")\n",
    "    mpl.ylabel(\"Weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((-0.2, 0.5))\n",
    "    mpl.show()\n",
    "    \n",
    "    #GMV Long-only\n",
    "    gmvlo = GMVLOPortfolio(cov)\n",
    "    gmvlo = pd.Series(gmvlo, index=index)\n",
    "    gmvlo = gmvlo.sort_values(ascending = False)\n",
    "    gmvlo.plot.bar(figsize=(15,7))\n",
    "    mpl.title(\"GMV Long-only\")\n",
    "    mpl.ylabel(\"weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((0,0.5))\n",
    "    mpl.show()\n",
    "    \n",
    "    \n",
    "    #Maximum_Div_port\n",
    "    mdv = pd.Series(max_div_port(cov), index=index)\n",
    "    mdv = mdv.sort_values(ascending=False)\n",
    "    mdv.plot.bar(figsize = (15,7))\n",
    "    mpl.title(\"Maximum Diversification Portfolio\")\n",
    "    mpl.ylabel(\"Weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((-0.1, 0.5))\n",
    "    mpl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports dates for the time-series\n",
    "dates = pd.read_excel('Betting Against Beta Equity Factors Daily.xlsx',\n",
    "                      'MKT',skiprows=18434,\n",
    "                      usecols='A', header = 0)\n",
    "dates.columns=['Date']\n",
    "dates=pd.Series(dates['Date'])\n",
    "dd= pd.to_datetime(dates[520:]).reset_index(drop=True)\n",
    "\n",
    "risk_free_rates = pd.read_excel('Betting Against Beta Equity Factors Daily.xlsx',\n",
    "                                'RF',skiprows=19001, usecols='B')\n",
    "risk_free_rates = (risk_free_rates[520:-18])\n",
    "#Imports all countries in the MKT factor\n",
    "mkt_data = pd.read_excel('Betting Against Beta Equity Factors Daily.xlsx','MKT',\n",
    "                         skiprows=18434, usecols='B:Y')\n",
    "mkt_global = pd.read_excel('Betting Against Beta Equity Factors Daily.xlsx','MKT',\n",
    "                           skiprows=18434, usecols='Z',\n",
    "                           header=0)\n",
    "mkt_data.columns = ['AUS', 'AUT', 'BEL', 'CAN', 'CHE', 'DEU', 'DNK', 'ESP', 'FIN',\n",
    "                    'FRA', 'GBR', 'GRC', 'HKG', 'IRL', 'ISR', 'ITA', 'JPN', 'NLD', \n",
    "                    'NOR', 'NZL', 'PRT', 'SGP', 'SWE', 'USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arp_data = mkt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main_2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funktioner til implementering af denoising af kovarians matrice\n",
    "#kun mpPDF er ændret lidt i forhold til bogen, da funktionerne ellers ikke virker\n",
    "#- - - - - - - -- - - - - - - - - - - - -- - - - - - - - - - - - - -- - - - - - - - - - - - -- - -\n",
    "def mpPDF(var,q,pts):\n",
    "    # Marcenko-Pastur pdf\n",
    "    # q=T/N\n",
    "    eMin,eMax=var*(1-(1./q)**.5)**2,var*(1+(1./q)**.5)**2\n",
    "    eVal=np.linspace(eMin,eMax,pts)\n",
    "    pdf=q/(2*np.pi*var*eVal)*((eMax-eVal)*(eVal-eMin))**.5\n",
    "    pdf = pdf.tolist()\n",
    "    pdf = list(itertools.chain.from_iterable(pdf))\n",
    "    pdf=pd.Series(pdf) #index=eVal\n",
    "    return pdf\n",
    "#- - - - - - - -- - - - - - - - - - - - -- - - - - - - - - - - - - -- - - - - - - - - - - - -- - -\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "#- - - - - - - -- - - - - - - - - - - - -- - - - - - - - - - - - - -- - - - - - - - - - - - -- - -\n",
    "def getPCA(matrix):\n",
    "    # Get eVal,eVec from a Hermitian matrix\n",
    "    eVal,eVec=np.linalg.eigh(matrix)\n",
    "    indices=eVal.argsort()[::-1] # arguments for sorting eVal desc\n",
    "    eVal,eVec=eVal[indices],eVec[:,indices]\n",
    "    eVal=np.diagflat(eVal)\n",
    "    return eVal,eVec\n",
    "#- - - - - - - -- - - - - - - - - - - - -- - - - - - - - - - - - - -- - - - - - - - - - - - -- - -\n",
    "def fitKDE(obs,bWidth=.25,kernel='gaussian',x=None):\n",
    "    # Fit kernel to a series of obs, and derive the prob of obs\n",
    "    # x is the array of values on which the fit KDE will be evaluated\n",
    "    if len(obs.shape)==1:obs=obs.reshape(-1,1)\n",
    "    kde=KernelDensity(kernel=kernel,bandwidth=bWidth).fit(obs)\n",
    "    if x is None:x=np.unique(obs).reshape(-1,1)\n",
    "    if len(x.shape)==1:x=x.reshape(-1,1)\n",
    "    logProb=kde.score_samples(x) # log(density)\n",
    "    pdf=pd.Series(np.exp(logProb),index=x.flatten())\n",
    "    return pdf\n",
    "\n",
    "#- - - - - - - -- - - - - - - - - - - - -- - - - - - - - - - - - - -- - - - - - - - - - - - -- - -\n",
    "def errPDFs(var,eVal,q,bWidth,pts=1000):\n",
    "    # Fit error\n",
    "    pdf0=mpPDF(var,q,pts) # theoretical pdf\n",
    "    pdf1=fitKDE(eVal,bWidth,x=pdf0.index.values) # empirical pdf\n",
    "    sse=np.sum((pdf1-pdf0)**2)\n",
    "    return sse\n",
    "#- - - - - - - -- - - - - - - - - - - - -- - - - - - - - - - - - - -- - - - - - - - - - - - -- - -\n",
    "def findMaxEval(eVal,q,bWidth):\n",
    "    # Find max random eVal by fitting Marcenko’s dist\n",
    "    out=minimize(lambda *x:errPDFs(*x),.5,args=(eVal,q,bWidth),\n",
    "        bounds=((0.00001,0.99999),))\n",
    "    if out['success']:var=out['x'][0]\n",
    "    else:var=1\n",
    "    eMax=var*(1+(1./q)**.5)**2\n",
    "    return eMax,var\n",
    "#- - - - - - - -- - - - - - - - - - - - -- - - - - - - - - - - - - -- - - - - - - - - - - - -- - -\n",
    "\n",
    "def denoisedCorr(eVal,eVec,nFacts):\n",
    "    # Remove noise from corr by fixing random eigenvalues\n",
    "    eVal_=np.diag(eVal).copy()\n",
    "    eVal_[nFacts:]=eVal_[nFacts:].sum()/float(eVal_.shape[0]-nFacts)\n",
    "    eVal_=np.diag(eVal_)\n",
    "    corr1=np.dot(eVec,eVal_).dot(eVec.T)\n",
    "    corr1=cov2corr(corr1)\n",
    "    return corr1\n",
    "\n",
    "#- - - - - - - -- - - - - - - - - - - - -- - - - - - - - - - - - - -- - - - - - - - - - - - -- - -\n",
    "def corr2cov(corr,std):\n",
    "    cov=corr*np.outer(std,std)\n",
    "    return cov\n",
    "\n",
    "def cov2corr(cov):\n",
    "    # Derive the correlation matrix from a covariance matrix\n",
    "    std=np.sqrt(np.diag(cov))\n",
    "    corr=cov/np.outer(std,std)\n",
    "    corr[corr<-1],corr[corr>1]=-1,1 # numerical error\n",
    "    return corr\n",
    "#- - - - - - - -- - - - - - - - - - - - -- - - - - - - - - - - - - -- - - - - - - - - - - - -- - -\n",
    "def deNoiseCov(cov0,q,bWidth):\n",
    "    corr0=cov2corr(cov0)\n",
    "    eVal0,eVec0=getPCA(corr0)\n",
    "    eMax0,var0=findMaxEval(np.diag(eVal0),q,bWidth)\n",
    "    nFacts0=eVal0.shape[0]-np.diag(eVal0)[::-1].searchsorted(eMax0)\n",
    "    corr1=denoisedCorr(eVal0,eVec0,nFacts0)\n",
    "    cov1=corr2cov(corr1,np.diag(cov0)**.5)\n",
    "    return cov1\n",
    "\n",
    "bWidth = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "from sklearn.metrics import mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funktioner til implementering af normalized mutual information\n",
    "def numBins(nObs,corr=None):\n",
    "    # Optimal number of bins for discretization\n",
    "    if corr is None: # univariate case\n",
    "        z=(8+324*nObs+12*(36*nObs+729*nObs**2)**.5)**(1/3.)\n",
    "        b=round(z/6.+2./(3*z)+1./3)\n",
    "    else: # bivariate case\n",
    "        b=round(2**-.5*(1+(1+24*nObs/(1.-corr**2))**.5)**.5)\n",
    "    return int(b)\n",
    "#- - - - - - - -- - - - - - - - - - - - -- - - - - - - - - - - - - -- - - - - - - - - - - - -- - -\n",
    "def varInfo(x,y,norm=False):\n",
    "    # variation of information\n",
    "    bXY=numBins(x.shape[0],corr=np.corrcoef(x,y)[0,1])\n",
    "    cXY=np.histogram2d(x,y,bXY)[0]\n",
    "    iXY=mutual_info_score(None,None,contingency=cXY)\n",
    "    hX=ss.entropy(np.histogram(x,bXY)[0]) # marginal\n",
    "    hY=ss.entropy(np.histogram(y,bXY)[0]) # marginal\n",
    "    vXY=hX+hY-2*iXY # variation of information\n",
    "    if norm:\n",
    "        hXY=hX+hY-iXY # joint\n",
    "        vXY/=hXY # normalized variation of information\n",
    "    return vXY\n",
    "#- - - - - - - -- - - - - - - - - - - - -- - - - - - - - - - - - - -- - - - - - - - - - - - -- - -\n",
    "def mutualInfo(x,y,norm=False):\n",
    "    # mutual information\n",
    "    bXY=numBins(x.shape[0],corr=np.corrcoef(x,y)[0,1])\n",
    "    cXY=np.histogram2d(x,y,bXY)[0]\n",
    "    iXY=mutual_info_score(None,None,contingency=cXY)\n",
    "    if norm:\n",
    "        hX=ss.entropy(np.histogram(x,bXY)[0]) # marginal\n",
    "        hY=ss.entropy(np.histogram(y,bXY)[0]) # marginal\n",
    "        iXY/=min(hX,hY) # normalized mutual information\n",
    "    return iXY\n",
    "#- - - - - - - -- - - - - - - - - - - - -- - - - - - - - - - - - - -- - - - - - - - - - - - -- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = arp_data.to_numpy()\n",
    "test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = test.shape[1]\n",
    "NMI = np.zeros((n, n))\n",
    "for ix in np.arange(n):\n",
    "    for jx in np.arange(ix+1,n):\n",
    "        NMI[ix,jx] = mutualInfo(test[:,ix], test[:,jx])\n",
    "        NMI[jx, ix] = NMI[ix, jx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMI = pd.DataFrame(NMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_3():\n",
    "    #Back-test on real-world data\n",
    "    re_calc_time,sample_size = 22,520\n",
    "    x = pd.DataFrame(arp_data)\n",
    "    #New\n",
    "    HRP_portfolio_return=[]\n",
    "    RP_portfolio_return=[]\n",
    "    RiskP_portfolio_return=[]\n",
    "    GMV_portfolio_return=[]\n",
    "    GMVLO_portfolio_return=[]\n",
    "    MD_portfolio_return=[]\n",
    "    EW_portfolio_return=[]\n",
    "    HRP2_portfolio_return=[]\n",
    "    \n",
    "    hrp_weights=[]\n",
    "    hrp2_weights=[]\n",
    "    gmv_weights=[]\n",
    "    mdv_weights=[]\n",
    "    ivp_weights=[]\n",
    "    rp_weights=[]\n",
    "    gmvlo_weights=[]\n",
    "    \n",
    "    realisedRC_RP = []\n",
    "    realisedRC_HRP = []\n",
    "    realisedRC_HRP2 = []\n",
    "    realisedRC_GMV = []\n",
    "    realisedRC_MD = []\n",
    "    realisedRC_IVP = []\n",
    "    realisedRC_GMVLO = []\n",
    "    realisedRC_EW = []\n",
    "    \n",
    "    realisedDR_RP = []\n",
    "    realisedDR_HRP = []\n",
    "    realisedDR_HRP2 = []\n",
    "    realisedDR_GMV = []\n",
    "    realisedDR_MD = []\n",
    "    realisedDR_IVP = []\n",
    "    realisedDR_GMVLO = []\n",
    "    realisedDR_EW = []\n",
    "    \n",
    "    EW_weight=[1/len(arp_data.columns)]*len(arp_data.columns)\n",
    "    returns=[]\n",
    "    pointers = range(520,len(x)-22,re_calc_time)\n",
    "    for pointer in pointers:\n",
    "        #Gets data_sample\n",
    "        x_sample = x.iloc[pointer-sample_size:pointer] \n",
    "        cov,corr=x_sample.cov().reset_index(drop=True) ,x_sample.corr().reset_index(drop=True)\n",
    "        cov=x_sample.cov().reset_index(drop=True)\n",
    "        \n",
    "        cov.columns, corr.columns = [i for i in range(len(cov))], [i for i in range(len(cov))]\n",
    "        cov = deNoiseCov(cov,len(arp_data)*1./cov.shape[1],bWidth) #cov denoise\n",
    "        corr = cov2corr(cov) #prøv evt. at kommenter den her ud, og kør med sample corr\n",
    "        \n",
    "        #HRP\n",
    "        dist=correlDist(corr)\n",
    "        dist.fillna(0, inplace=True)\n",
    "        link=sch.linkage(dist,'single')\n",
    "        sortIx=getQuasiDiag(link)\n",
    "        sortIx=NMI.index[sortIx].tolist() #recover labels\n",
    "       # print(\"sort IX: {}: \".format(sortIx))\n",
    "        #df0=corr.loc[sortIx,sortIx]\n",
    "        #plotCorrMatrix('HRP3_corr{}.png'.format(i),df0,labels=df0.columns)\n",
    "       # print(\"hrp inden sort_index: {}:\".format(getRecBipart(cov,sortIx)))\n",
    "        hrp=getRecBipart(cov,sortIx).sort_index()\n",
    "       # print(\"hrp efter sort_index: {}\".format(hrp))\n",
    "        #HRP2\n",
    "        cluster_dict = get_cluster_dict(link)\n",
    "       # print(cluster_dict)\n",
    "        hrp2 = recClusterVar(cluster_dict,link, cov)#.sort_index()\n",
    "       # print(\"hrp2 efter sort_index: {}\".format(hrp2))\n",
    "        #IVP\n",
    "        ivp = getIVP(cov)\n",
    "        #GMV\n",
    "        gmv = GMVPortfolio(cov)\n",
    "        #GMV Long-only\n",
    "        gmvlo = GMVLOPortfolio(cov)\n",
    "        #MD\n",
    "        mdv = pd.Series(max_div_port(cov))\n",
    "        #Risk Parity\n",
    "        rp = pd.Series(risk_parity(cov))\n",
    "        hrp_weights.append(hrp)\n",
    "       # if pointer>520:\n",
    "        #    print(\"Sum af hrp-turnover : {}\".format(sum(abs(hrp_weights[int((pointer-520)/22)]-hrp_weights[int((pointer-520)/22)-1]))))\n",
    "        hrp2_weights.append(hrp2)\n",
    "        gmv_weights.append(gmv)\n",
    "        gmvlo_weights.append(gmvlo)\n",
    "        mdv_weights.append(mdv)\n",
    "        ivp_weights.append(ivp)\n",
    "        rp_weights.append(rp)\n",
    "        \n",
    "        #realised risk contribution\n",
    "        cov_out = x.iloc[pointer:pointer+re_calc_time].cov()\n",
    "        realisedRC_RP.append(calculate_risk_contribution(rp, cov_out)/\n",
    "                             np.sqrt(calculate_portfolio_var(rp,cov_out)))\n",
    "        realisedRC_HRP.append(calculate_risk_contribution(hrp, cov_out)/\n",
    "                              np.sqrt(calculate_portfolio_var(hrp,cov_out)))\n",
    "        realisedRC_HRP2.append(calculate_risk_contribution(hrp2, cov_out)/\n",
    "                               np.sqrt(calculate_portfolio_var(hrp2,cov_out)))\n",
    "        realisedRC_GMV.append(calculate_risk_contribution(gmv, cov_out)/\n",
    "                              np.sqrt(calculate_portfolio_var(gmv,cov_out)))\n",
    "        realisedRC_MD.append(calculate_risk_contribution(mdv, cov_out)/\n",
    "                             np.sqrt(calculate_portfolio_var(mdv,cov_out)))\n",
    "        realisedRC_IVP.append(calculate_risk_contribution(ivp, cov_out)/\n",
    "                              np.sqrt(calculate_portfolio_var(ivp,cov_out)))\n",
    "        realisedRC_GMVLO.append(calculate_risk_contribution(gmvlo, cov_out)/\n",
    "                                np.sqrt(calculate_portfolio_var(gmvlo,cov_out)))\n",
    "        realisedRC_EW.append(calculate_risk_contribution(EW_weight, cov_out)/\n",
    "                             np.sqrt(calculate_portfolio_var(pd.Series(EW_weight),\n",
    "                                                             cov_out)))\n",
    "        \n",
    "        \n",
    "        #realised diversification ratio\n",
    "        realisedDR_RP.append(calc_diversification_ratio(rp, cov_out))\n",
    "        realisedDR_HRP.append(calc_diversification_ratio(hrp, cov_out))\n",
    "        realisedDR_HRP2.append(calc_diversification_ratio(hrp2, cov_out))\n",
    "        realisedDR_GMV.append(calc_diversification_ratio(gmv, cov_out))\n",
    "        realisedDR_MD.append(calc_diversification_ratio(mdv, cov_out))\n",
    "        realisedDR_IVP.append(calc_diversification_ratio(ivp, cov_out))\n",
    "        realisedDR_GMVLO.append(calc_diversification_ratio(gmvlo, cov_out))\n",
    "        realisedDR_EW.append(calc_diversification_ratio(pd.Series(EW_weight),\n",
    "                                                        cov_out))\n",
    "        \n",
    "        for j in range(re_calc_time):\n",
    "            HRP_portfolio_return.append(np.dot(hrp,arp_data.iloc[pointer+j]))\n",
    "            RP_portfolio_return.append(np.dot(ivp,arp_data.iloc[pointer+j]))\n",
    "            GMV_portfolio_return.append(np.dot(gmv,arp_data.iloc[pointer+j]))\n",
    "            GMVLO_portfolio_return.append(np.dot(gmvlo,arp_data.iloc[pointer+j]))\n",
    "            MD_portfolio_return.append(np.dot(mdv,arp_data.iloc[pointer+j]))\n",
    "            EW_portfolio_return.append(np.dot(EW_weight,arp_data.iloc[pointer+j]))\n",
    "            HRP2_portfolio_return.append(np.dot(hrp2,arp_data.iloc[pointer+j]))\n",
    "            RiskP_portfolio_return.append(np.dot(rp,arp_data.iloc[pointer+j]))\n",
    "    return (pd.DataFrame(HRP_portfolio_return), pd.DataFrame(RP_portfolio_return), \n",
    "            pd.DataFrame(GMV_portfolio_return), pd.DataFrame(GMVLO_portfolio_return), \n",
    "            pd.DataFrame(MD_portfolio_return),pd.DataFrame(EW_portfolio_return), \n",
    "            pd.DataFrame(HRP2_portfolio_return), pd.DataFrame(RiskP_portfolio_return), \n",
    "            pd.DataFrame(returns),pd.DataFrame(hrp_weights),pd.DataFrame(hrp2_weights),\n",
    "            pd.DataFrame(gmv_weights),\n",
    "            pd.DataFrame(mdv_weights),pd.DataFrame(ivp_weights),\n",
    "            pd.DataFrame(rp_weights), pd.DataFrame(gmvlo_weights),\n",
    "            realisedRC_RP, realisedRC_HRP, \n",
    "            realisedRC_HRP2, realisedRC_GMV, realisedRC_GMVLO,\n",
    "            realisedRC_IVP, realisedRC_MD, \n",
    "            realisedRC_EW, realisedDR_GMV,realisedDR_GMVLO,\n",
    "             realisedDR_HRP,realisedDR_HRP2,realisedDR_IVP,\n",
    "            realisedDR_MD,realisedDR_RP,realisedDR_EW)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_at_risk(returns, confidence_level=.05):\n",
    "    return returns.quantile(confidence_level, interpolation='higher')\n",
    "\n",
    "\n",
    "def expected_shortfall(returns, confidence_level=.05):\n",
    "    var = value_at_risk(returns, confidence_level)\n",
    "\n",
    "    return returns[returns.lt(var)].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(HRP_weights,RP_weights,GMV_weights,GMVLO_weights,\n",
    " MD_weights,EW_weights,HRP2_weights, RiskP_weights,\n",
    " returns, hrp_weights, hrp2_weights, gmv_weights, \n",
    " mdv_weights, ivp_weights, rp_weights, gmvlo_weights, \n",
    " realisedRC_RP, realisedRC_HRP, realisedRC_HRP2, \n",
    " realisedRC_GMV, realisedRC_GMVLO, realisedRC_IVP, \n",
    " realisedRC_MD, realisedRC_EW, realisedDR_GMV, \n",
    " realisedDR_GMVLO, realisedDR_HRP, realisedDR_HRP2, \n",
    " realisedDR_IVP, realisedDR_MD, realisedDR_RP, realisedDR_EW) = main_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the cummulative returns\n",
    "HRP_portfolio_returns = HRP_weights\n",
    "HRP_cumulative_returns = (HRP_portfolio_returns + 1).cumprod()\n",
    "HRP_cumulative_returns['dates']=dd\n",
    "HRP_cumulative_returns.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "HRP2_portfolio_returns = HRP2_weights\n",
    "HRP2_cumulative_returns = (HRP2_portfolio_returns +1).cumprod()\n",
    "HRP2_cumulative_returns['dates']=dd\n",
    "HRP2_cumulative_returns.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "RP_portfolio_returns = RP_weights\n",
    "RP_cumulative_returns = (RP_portfolio_returns + 1).cumprod()\n",
    "RP_cumulative_returns['dates']=dd\n",
    "RP_cumulative_returns.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "RiskP_portfolio_returns = RiskP_weights\n",
    "RiskP_cumulative_returns = (RiskP_portfolio_returns + 1).cumprod()\n",
    "RiskP_cumulative_returns['dates']=dd\n",
    "RiskP_cumulative_returns.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "GMV_portfolio_returns = GMV_weights\n",
    "GMV_cumulative_returns = (GMV_portfolio_returns + 1).cumprod()\n",
    "GMV_cumulative_returns['dates']=dd\n",
    "GMV_cumulative_returns.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "GMVLO_portfolio_returns = GMVLO_weights\n",
    "GMVLO_cumulative_returns = (GMVLO_portfolio_returns + 1).cumprod()\n",
    "GMVLO_cumulative_returns['dates']=dd\n",
    "GMVLO_cumulative_returns.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "MD_portfolio_returns = MD_weights\n",
    "MD_cumulative_returns = (MD_portfolio_returns + 1).cumprod()\n",
    "MD_cumulative_returns['dates']=dd\n",
    "MD_cumulative_returns.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "EW_portfolio_returns = EW_weights\n",
    "EW_cumulative_returns = (EW_portfolio_returns + 1).cumprod()\n",
    "EW_cumulative_returns['dates']=dd\n",
    "EW_cumulative_returns.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "mkt_global_cumulative_returns = (mkt_global[520:] + 1).cumprod()\n",
    "mkt_global_cumulative_returns =mkt_global_cumulative_returns.reset_index(drop=True)\n",
    "mkt_global_cumulative_returns['dates']=dd\n",
    "mkt_global_cumulative_returns.set_index('dates',inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot cummulative returns of strategies\n",
    "ax = HRP_cumulative_returns.plot(figsize=[15,10])\n",
    "RP_cumulative_returns.plot(figsize=[15,10], ax=ax)\n",
    "RiskP_cumulative_returns.plot(figsize=[15,10],ax=ax)\n",
    "GMVLO_cumulative_returns.plot(figsize=[15,10], ax=ax)\n",
    "MD_cumulative_returns.plot(figsize=[15,10], ax=ax)\n",
    "EW_cumulative_returns.plot(figsize=[15,10], ax=ax)\n",
    "HRP2_cumulative_returns.plot(figsize=[15,10], ax=ax)\n",
    "\n",
    "mkt_global_cumulative_returns.plot(figsize=[15,10], ax=ax)\n",
    "\n",
    "mpl.legend(['HRP', 'Naive RP','RP', 'GMV', 'MD','EW','HRP2', 'Global'])\n",
    "mpl.ylabel(\"Accumulated returns (Index 1)\")\n",
    "mpl.yscale(\"log\")\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates table to show the results\n",
    "l=len(HRP_cumulative_returns)-1\n",
    "allo_comp = [[(HRP_cumulative_returns[0][-1])**(1/23)-1,\n",
    "              (HRP2_cumulative_returns[0][-1])**(1/23)-1, \n",
    "              (RP_cumulative_returns[0][-1])**(1/23)-1,\n",
    "              (GMV_cumulative_returns[0][-1])**(1/23)-1, \n",
    "              (GMVLO_cumulative_returns[0][-1])**(1/23)-1,\n",
    "              (MD_cumulative_returns[0][-1])**(1/23)-1,\n",
    "              (RiskP_cumulative_returns[0][-1])**(1/23)-1 ,\n",
    "              (EW_cumulative_returns[0][-1])**(1/23)-1]]\n",
    "allo_comp.append([HRP_portfolio_returns.std()[0]*np.sqrt(260),\n",
    "                  HRP2_portfolio_returns.std()[0]*np.sqrt(260),\n",
    "                  RP_portfolio_returns.std()[0]*np.sqrt(260), \n",
    "                  GMV_portfolio_returns.std()[0]*np.sqrt(260), \n",
    "                  GMVLO_portfolio_returns.std()[0]*np.sqrt(260),\n",
    "                  MD_portfolio_returns.std()[0]*np.sqrt(260),\n",
    "                  RiskP_portfolio_returns.std()[0]*np.sqrt(260),\n",
    "                  EW_portfolio_returns.std()[0]*np.sqrt(260)])\n",
    "allo_comp.append([expected_shortfall(HRP_portfolio_returns)[0],\n",
    "                  expected_shortfall(HRP2_portfolio_returns)[0],\n",
    "                  expected_shortfall(RP_portfolio_returns)[0], \n",
    "                  expected_shortfall(GMV_portfolio_returns)[0], \n",
    "                  expected_shortfall(GMVLO_portfolio_returns)[0], \n",
    "                  expected_shortfall(MD_portfolio_returns)[0],\n",
    "                  expected_shortfall(RiskP_portfolio_returns)[0], \n",
    "                  expected_shortfall(EW_portfolio_returns)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allo_comp=pd.DataFrame(allo_comp, columns=['HRP', 'HRP2','Naive RP','GMV','GMVLO','MD','Risk Parity','EW'],\n",
    "                       index=['Annualized return','Annualized standard deviance','Expected shortfall'])\n",
    "#Adds Sharpe Ratio to the table\n",
    "allo_comp.loc['Sharpe Ratio']=allo_comp.loc['Annualized return']/allo_comp.loc['Annualized standard deviance']\n",
    "#Expected shortfall N days\n",
    "N = 22\n",
    "\n",
    "HRP_portfolio_returns14 = HRP_portfolio_returns.groupby(HRP_portfolio_returns.index // N).sum()\n",
    "HRP2_portfolio_returns14 = HRP2_portfolio_returns.groupby(HRP2_portfolio_returns.index // N).sum()\n",
    "RP_portfolio_returns14 = RP_portfolio_returns.groupby(RP_portfolio_returns.index // N).sum()\n",
    "GMV_portfolio_returns14 = GMV_portfolio_returns.groupby(GMV_portfolio_returns.index // N).sum()\n",
    "GMVLO_portfolio_returns14 = GMVLO_portfolio_returns.groupby(GMVLO_portfolio_returns.index // N).sum()\n",
    "MD_portfolio_returns14 = MD_portfolio_returns.groupby(MD_portfolio_returns.index // N).sum()\n",
    "RiskP_portfolio_returns14 = RiskP_portfolio_returns.groupby(RiskP_portfolio_returns.index // N).sum()\n",
    "EW_portfolio_returns14 = EW_portfolio_returns.groupby(EW_portfolio_returns.index // N).sum()\n",
    "allo_comp.loc['Expected shortfall 14 days'] = [expected_shortfall(HRP_portfolio_returns14)[0],\n",
    "                                               expected_shortfall(HRP2_portfolio_returns14)[0],\n",
    "                                               expected_shortfall(RP_portfolio_returns14)[0],\n",
    "                                               expected_shortfall(GMV_portfolio_returns14)[0],\n",
    "                                               expected_shortfall(GMVLO_portfolio_returns14)[0],\n",
    "                                               expected_shortfall(MD_portfolio_returns14)[0],\n",
    "                                               expected_shortfall(RiskP_portfolio_returns14)[0],\n",
    "                                               expected_shortfall(EW_portfolio_returns14)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adds realised risk contribution and diversification ratio to the table\n",
    "allo_comp.loc['Realised deviance risk contribution'] = [portfolio_risk_deviance(realisedRC_HRP),\n",
    "                                                        portfolio_risk_deviance(realisedRC_HRP2),\n",
    "                                                        portfolio_risk_deviance(realisedRC_IVP),\n",
    "                                                        portfolio_risk_deviance(realisedRC_GMV),\n",
    "                                                        portfolio_risk_deviance(realisedRC_GMVLO),\n",
    "                                                        portfolio_risk_deviance(realisedRC_MD),\n",
    "                                                        portfolio_risk_deviance(realisedRC_RP),\n",
    "                                                        portfolio_risk_deviance(realisedRC_EW)]\n",
    "allo_comp.loc['Realised diversification ratio'] = [abs(statistics.mean(realisedDR_HRP)),\n",
    "                                                   abs(statistics.mean(realisedDR_HRP2)),\n",
    "                                                   abs(statistics.mean(realisedDR_IVP)),\n",
    "                                                   abs(statistics.mean(realisedDR_GMV)),\n",
    "                                                   abs(statistics.mean(realisedDR_GMVLO)),\n",
    "                                                   abs(statistics.mean(realisedDR_MD)),\n",
    "                                                   abs(statistics.mean(realisedDR_RP)),\n",
    "                                                   abs(statistics.mean(realisedDR_EW))]\n",
    "allo_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1\n",
    "returns22 += 1\n",
    "x_hat_hrp = pd.DataFrame(returns22[25:].values*hrp_weights.values, columns=returns22.columns, index=hrp_weights.index)\n",
    "x_hat_hrp2 = pd.DataFrame(returns22[25:].values*hrp2_weights.values, columns=returns22.columns, index=hrp2_weights.index)\n",
    "x_hat_ivp = pd.DataFrame(returns22[25:].values*ivp_weights.values, columns=returns22.columns, index=ivp_weights.index)\n",
    "x_hat_gmv = pd.DataFrame(returns22[25:].values*gmvlo_weights.values, columns=returns22.columns, index=gmvlo_weights.index)\n",
    "x_hat_md = pd.DataFrame(returns22[25:].values*mdv_weights.values, columns=returns22.columns, index=mdv_weights.index)\n",
    "x_hat_rp = pd.DataFrame(returns22[25:].values*rp_weights.values, columns=returns22.columns, index=rp_weights.index)\n",
    "\n",
    "ew_weights = pd.DataFrame(1/24,index=hrp_weights.index, columns=hrp_weights.columns)\n",
    "x_hat_ew = pd.DataFrame(returns22[25:].values*ew_weights.values, columns=returns22.columns, index=ew_weights.index)\n",
    "\n",
    "#step 2\n",
    "q_hrp = x_hat_hrp.sum(axis=1)\n",
    "x_tilde_hrp = x_hat_hrp.div(q_hrp, axis=0)\n",
    "\n",
    "q_hrp2 = x_hat_hrp2.sum(axis=1)\n",
    "x_tilde_hrp2 = x_hat_hrp2.div(q_hrp2, axis=0)\n",
    "\n",
    "q_ivp = x_hat_ivp.sum(axis=1)\n",
    "x_tilde_ivp = x_hat_ivp.div(q_ivp, axis=0)\n",
    "\n",
    "q_gmv = x_hat_gmv.sum(axis=1)\n",
    "x_tilde_gmv = x_hat_gmv.div(q_gmv, axis=0)\n",
    "\n",
    "q_md = x_hat_md.sum(axis=1)\n",
    "x_tilde_md = x_hat_md.div(q_md, axis=0)\n",
    "\n",
    "q_rp = x_hat_rp.sum(axis=1)\n",
    "x_tilde_rp = x_hat_rp.div(q_rp, axis=0)\n",
    "\n",
    "q_ew = x_hat_ew.sum(axis=1)\n",
    "x_tilde_ew = x_hat_ew.div(q_ew, axis=0)\n",
    "\n",
    "\n",
    "#step 3\n",
    "HRP_Turnover = pd.DataFrame(abs(x_tilde_hrp.iloc[:272].values - hrp_weights.iloc[1:].values),\n",
    "                            columns=returns22.columns, index=x_tilde_hrp.iloc[:272].index)\n",
    "HRP_Turnover = HRP_Turnover.sum(axis=1)\n",
    "\n",
    "HRP2_Turnover = pd.DataFrame(abs(x_tilde_hrp2.iloc[:272].values - hrp2_weights.iloc[1:].values),\n",
    "                             columns=returns22.columns, index=x_tilde_hrp2.iloc[:272].index)\n",
    "HRP2_Turnover = HRP2_Turnover.sum(axis=1)\n",
    "\n",
    "IVP_Turnover = pd.DataFrame(abs(x_tilde_ivp.iloc[:272].values - ivp_weights.iloc[1:].values),\n",
    "                            columns=returns22.columns, index=x_tilde_ivp.iloc[:272].index)\n",
    "IVP_Turnover = IVP_Turnover.sum(axis=1)\n",
    "\n",
    "GMV_Turnover = pd.DataFrame(abs(x_tilde_gmv.iloc[:272].values - gmvlo_weights.iloc[1:].values),\n",
    "                            columns=returns22.columns, index=x_tilde_gmv.iloc[:272].index)\n",
    "GMV_Turnover = GMV_Turnover.sum(axis=1)\n",
    "\n",
    "MD_Turnover = pd.DataFrame(abs(x_tilde_md.iloc[:272].values - mdv_weights.iloc[1:].values),\n",
    "                           columns=returns22.columns, index=x_tilde_md.iloc[:272].index)\n",
    "MD_Turnover = MD_Turnover.sum(axis=1)\n",
    "\n",
    "RP_Turnover = pd.DataFrame(abs(x_tilde_rp.iloc[:272].values - rp_weights.iloc[1:].values),\n",
    "                           columns=returns22.columns, index=x_tilde_rp.iloc[:272].index)\n",
    "RP_Turnover = RP_Turnover.sum(axis=1)\n",
    "\n",
    "EW_Turnover = pd.DataFrame(abs(x_tilde_ew.iloc[:272].values - ew_weights.iloc[1:].values), columns=returns22.columns, index=x_tilde_ew.iloc[:272].index)\n",
    "EW_Turnover = EW_Turnover.sum(axis=1)\n",
    "\n",
    "#Transaction cost\n",
    "HRP_xi = HRP_cumulative_returns.iloc[22::22].mul(0.001)\n",
    "HRP2_xi = HRP2_cumulative_returns.iloc[22::22].mul(0.001)\n",
    "IVP_xi = RP_cumulative_returns.iloc[22::22].mul(0.001)\n",
    "GMV_xi = GMV_cumulative_returns.iloc[22::22].mul(0.001)\n",
    "MD_xi = MD_cumulative_returns.iloc[22::22].mul(0.001)\n",
    "RP_xi = RiskP_cumulative_returns.iloc[22::22].mul(0.001)\n",
    "EW_xi = EW_cumulative_returns.iloc[22::22].mul(0.001)\n",
    "\n",
    "def flatlist(l):\n",
    "    flat_list = []\n",
    "    for sublist in l.values.tolist():\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    return pd.Series(flat_list)\n",
    "\n",
    "HRP_xi = flatlist(HRP_xi)\n",
    "HRP_TC = pd.DataFrame((HRP_Turnover.values*HRP_xi.values))\n",
    "\n",
    "HRP2_xi = flatlist(HRP2_xi)\n",
    "HRP2_TC = pd.DataFrame((HRP2_Turnover.values*HRP2_xi.values))\n",
    "\n",
    "IVP_xi = flatlist(IVP_xi)\n",
    "IVP_TC = pd.DataFrame((IVP_Turnover.values*IVP_xi.values))\n",
    "\n",
    "GMV_xi = flatlist(GMV_xi)\n",
    "GMV_TC = pd.DataFrame((GMV_Turnover.values*GMV_xi.values))\n",
    "\n",
    "MD_xi = flatlist(MD_xi)\n",
    "MD_TC = pd.DataFrame((MD_Turnover.values*MD_xi.values))\n",
    "\n",
    "RP_xi = flatlist(RP_xi)\n",
    "RP_TC = pd.DataFrame((RP_Turnover.values*RP_xi.values))\n",
    "\n",
    "EW_xi = flatlist(EW_xi)\n",
    "EW_TC = pd.DataFrame((EW_Turnover.values*EW_xi.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subtracting transaction cost every month\n",
    "HRP_returns_TC = HRP_portfolio_returns\n",
    "HRP2_returns_TC = HRP2_portfolio_returns\n",
    "IVP_returns_TC = RP_portfolio_returns\n",
    "GMV_returns_TC = GMVLO_portfolio_returns\n",
    "MD_returns_TC = MD_portfolio_returns\n",
    "RP_returns_TC = RiskP_portfolio_returns\n",
    "EW_returns_TC = EW_portfolio_returns\n",
    "\n",
    "i=0\n",
    "k=0\n",
    "\n",
    "while (k<272):\n",
    "    HRP_returns_TC.iloc[i] = HRP_portfolio_returns.iloc[i].sub(HRP_TC.iloc[k])\n",
    "    HRP2_returns_TC.iloc[i] = HRP2_portfolio_returns.iloc[i].sub(HRP2_TC.iloc[k])\n",
    "    IVP_returns_TC.iloc[i] = RP_portfolio_returns.iloc[i].sub(IVP_TC.iloc[k])\n",
    "    GMV_returns_TC.iloc[i] = GMVLO_portfolio_returns.iloc[i].sub(GMV_TC.iloc[k])\n",
    "    MD_returns_TC.iloc[i] = MD_portfolio_returns.iloc[i].sub(MD_TC.iloc[k])\n",
    "    RP_returns_TC.iloc[i] = RiskP_portfolio_returns.iloc[i].sub(RP_TC.iloc[k])\n",
    "    EW_returns_TC.iloc[i] = EW_portfolio_returns.iloc[i].sub(EW_TC.iloc[k])\n",
    "    i += 22\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cumulative returns with transaction cost\n",
    "HRP_cumulative_returns_TC = (HRP_returns_TC + 1).cumprod()\n",
    "HRP2_cumulative_returns_TC = (HRP2_returns_TC + 1).cumprod()\n",
    "IVP_cumulative_returns_TC = (IVP_returns_TC + 1).cumprod()\n",
    "GMV_cumulative_returns_TC = (GMV_returns_TC + 1).cumprod()\n",
    "MD_cumulative_returns_TC = (MD_returns_TC + 1).cumprod()\n",
    "RP_cumulative_returns_TC = (RP_returns_TC + 1).cumprod()\n",
    "EW_cumulative_returns_TC = (EW_returns_TC + 1).cumprod()\n",
    "\n",
    "#Date\n",
    "HRP_cumulative_returns_TC['dates']=dd\n",
    "HRP_cumulative_returns_TC.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "HRP2_cumulative_returns_TC['dates']=dd\n",
    "HRP2_cumulative_returns_TC.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "IVP_cumulative_returns_TC['dates']=dd\n",
    "IVP_cumulative_returns_TC.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "GMV_cumulative_returns_TC['dates']=dd\n",
    "GMV_cumulative_returns_TC.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "MD_cumulative_returns_TC['dates']=dd\n",
    "MD_cumulative_returns_TC.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "RP_cumulative_returns_TC['dates']=dd\n",
    "RP_cumulative_returns_TC.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "EW_cumulative_returns_TC['dates']=dd\n",
    "EW_cumulative_returns_TC.set_index('dates',inplace=True,drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot of cumulative returns with transaction costs\n",
    "ax = HRP_cumulative_returns_TC.plot(figsize=[15,10])\n",
    "IVP_cumulative_returns_TC.plot(figsize=[15,10], ax=ax)\n",
    "RP_cumulative_returns_TC.plot(figsize=[15,10],ax=ax)\n",
    "GMV_cumulative_returns_TC.plot(figsize=[15,10], ax=ax)\n",
    "MD_cumulative_returns_TC.plot(figsize=[15,10], ax=ax)\n",
    "EW_cumulative_returns_TC.plot(figsize=[15,10], ax=ax)\n",
    "HRP2_cumulative_returns_TC.plot(figsize=[15,10], ax=ax)\n",
    "\n",
    "mpl.legend(['HRP', 'Naive RP','RP', 'GMV', 'MD','EW', 'HRP2'])\n",
    "mpl.ylabel(\"Accumulated returns (Index 1)\")\n",
    "mpl.yscale('log')\n",
    "mpl.savefig('Cumulative returns TC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annualized turnover\n",
    "HRP_Turnover_ann = HRP_Turnover.groupby(HRP_Turnover.index // 12).sum()\n",
    "HRP2_Turnover_ann = HRP2_Turnover.groupby(HRP2_Turnover.index // 12).sum()\n",
    "IVP_Turnover_ann = IVP_Turnover.groupby(IVP_Turnover.index // 12).sum()\n",
    "GMV_Turnover_ann = GMV_Turnover.groupby(GMV_Turnover.index // 12).sum()\n",
    "MD_Turnover_ann = MD_Turnover.groupby(MD_Turnover.index // 12).sum()\n",
    "RP_Turnover_ann = RP_Turnover.groupby(RP_Turnover.index // 12).sum()\n",
    "EW_Turnover_ann = EW_Turnover.groupby(EW_Turnover.index // 12).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turnover table\n",
    "Turnover_table = [[HRP_Turnover_ann.max(), HRP2_Turnover_ann.max(), IVP_Turnover_ann.max(), GMV_Turnover_ann.max(), MD_Turnover_ann.max(), RP_Turnover_ann.max(), EW_Turnover_ann.max()]]\n",
    "Turnover_table.append([HRP_Turnover_ann.min(), HRP2_Turnover_ann.min(), IVP_Turnover_ann.min(), GMV_Turnover_ann.min(), MD_Turnover_ann.min(), RP_Turnover_ann.min(), EW_Turnover_ann.min()])\n",
    "Turnover_table.append([HRP_Turnover_ann.mean(), HRP2_Turnover_ann.mean(), IVP_Turnover_ann.mean(), GMV_Turnover_ann.mean(), MD_Turnover_ann.mean(), RP_Turnover_ann.mean(), EW_Turnover_ann.mean()])\n",
    "Turnover_table.append([HRP_TC.sum()[0], HRP2_TC.sum()[0], IVP_TC.sum()[0], GMV_TC.sum()[0], MD_TC.sum()[0], RP_TC.sum()[0], EW_TC.sum()[0]])\n",
    "Turnover_table = pd.DataFrame(Turnover_table, columns=['HRP', 'HRP2','Naive RP','GMV','MD','Risk Parity', 'EW'], index=['Max. turnover', 'Min. turnover', 'Avg. turnover', 'total transaction cost'])\n",
    "Turnover_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#HRP_weights.columns = arp_data.columns\n",
    "weight_stats=pd.DataFrame()\n",
    "weight_stats['mean'] = HRP_weights.mean()\n",
    "weight_stats['std.dev'] = HRP_weights.std()\n",
    "weight_stats['variance'] = HRP_weights.var()\n",
    "# weight_stats.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrp_weights.tail(1).plot.bar(figsize = (15,7))\n",
    "mpl.title(\"HRP\")\n",
    "mpl.ylabel(\"Weight\")\n",
    "mpl.xlabel(\"Asset\")\n",
    "mpl.ylim()\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rene_strategier=(arp_data.iloc[262:]+1).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rene_strategier.plot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmv_weights.plot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrp2_weights.plot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrp_weights.plot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdv_weights.plot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rp_weights.plot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmvlo_weights.plot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Monte Carlo experiment\n",
    "import time\n",
    "\n",
    "def generateData(nObs,sLength,size0,size1,mu0,sigma0,sigma1F):\n",
    "    # Time series of correlated variables\n",
    "    #1) generate random uncorrelated data\n",
    "    x=np.random.normal(mu0,sigma0,size=(nObs,size0)) # each row is a variable \n",
    "    #2) create correlation between the variables\n",
    "    cols=[random.randint(0,size0-1) for i in range(size1)]\n",
    "    y=x[:,cols]+np.random.normal(0,sigma0*sigma1F,size=(nObs,len(cols)))\n",
    "    x=np.append(x,y,axis=1)\n",
    "    #3) add common random shock\n",
    "    point=np.random.randint(sLength,nObs-1,size=2)\n",
    "    x[np.ix_(point,[cols[0],size0])]=np.array([[-.5,-.5],[2,2]])\n",
    "    #4) add specific random shock\n",
    "    point=np.random.randint(sLength,nObs-1,size=2)\n",
    "    x[point,cols[-1]]=np.array([-.5,2])\n",
    "    return x,cols\n",
    "\n",
    "def getHRP(cov,corr):\n",
    "    # Construct a hierarchical portfolio\n",
    "    corr,cov=pd.DataFrame(corr),pd.DataFrame(cov)\n",
    "    dist=correlDist(corr)\n",
    "    link=sch.linkage(dist,'single')\n",
    "    sortIx=getQuasiDiag(link) \n",
    "    sortIx=corr.index[sortIx].tolist() # recover labels \n",
    "    hrp=getRecBipart(cov,sortIx)\n",
    "    return hrp.sort_index()\n",
    "\n",
    "def getHRP2(cov,corr):\n",
    "    # Construct a hierarchical portfolio\n",
    "    corr,cov=pd.DataFrame(corr),pd.DataFrame(cov)\n",
    "    dist=correlDist(corr) \n",
    "    link=sch.linkage(dist,'single')\n",
    "    cluster_dict = get_cluster_dict(link)\n",
    "    hrp2 = recClusterVar(cluster_dict,link, cov)\n",
    "    return hrp2\n",
    "\n",
    "\n",
    "def hrpMC(numIters=20,nObs=520,size0=5,size1=5,mu0=0,sigma0=0.01, sigma1F=.25,sLength=260,rebal=22):\n",
    "    start_time = time.time()\n",
    "    # Monte Carlo experiment on HRP\n",
    "    methods=[getHRP,getHRP2,getIVP,risk_parity,\n",
    "             GMVPortfolio,GMVLOPortfolio,max_div_port,ewPortfolio]#,getCLA] \n",
    "    stats,numIter={i.__name__:pd.Series() for i in methods},0\n",
    "    pointers=range(sLength,nObs,rebal)\n",
    "    divratio={i.__name__:pd.Series() for i in methods}\n",
    "    rc={i.__name__:pd.Series() for i in methods}\n",
    "    w={i.__name__:pd.DataFrame() for i in methods}\n",
    "    while numIter<numIters:\n",
    "        #1) Prepare data for one experiment \n",
    "        x,cols=generateData(nObs,sLength,size0,size1,mu0,sigma0,sigma1F)\n",
    "        r={i.__name__:pd.Series() for i in methods}\n",
    "        #2) Compute portfolios in-sample\n",
    "        for pointer in pointers:\n",
    "            x_=x[pointer-sLength:pointer]\n",
    "            cov_,corr_=np.cov(x_,rowvar=0),np.corrcoef(x_,rowvar=0) \n",
    "            #3) Compute performance out-of-sample\n",
    "            x_=x[pointer:pointer+rebal]\n",
    "            cov_out=pd.DataFrame(np.cov(x[pointer:pointer+rebal],rowvar=0))\n",
    "            for func in methods:\n",
    "                w_=pd.Series(func(cov=cov_,corr=corr_))\n",
    "                # callback\n",
    "                w[func.__name__]=w[func.__name__].append(w_,ignore_index=True)\n",
    "                r_=pd.Series(np.dot(x_,w_))\n",
    "                divratio_ = calc_diversification_ratio(w_,cov_out)\n",
    "                divratio[func.__name__]=divratio[func.__name__].append(pd.Series(divratio_))\n",
    "                rc_ = np.squeeze(np.asarray(calculate_risk_contribution(w_, cov_out)))\n",
    "                        /np.sqrt(calculate_portfolio_var(w_,cov_out))\n",
    "                rc[func.__name__]=rc[func.__name__].append(pd.Series(rc_))\n",
    "                r[func.__name__]=r[func.__name__].append(r_)\n",
    "        #4) Evaluate and store results\n",
    "        for func in methods:\n",
    "            r_=r[func.__name__].reset_index(drop=True)\n",
    "            p_=(1+r_).cumprod()\n",
    "            stats[func.__name__].loc[numIter]=p_.iloc[-1]-1 # terminal return\n",
    "        numIter+=1\n",
    "    #5) Report results\n",
    "    stats=pd.DataFrame.from_dict(stats,orient='columns')\n",
    "    stats.to_csv('stats.csv')\n",
    "    df0,df1=stats.std(),stats.var()\n",
    "    print(pd.concat([df0,df1,df1/df1['getHRP']-1],axis=1))\n",
    "    return divratio,rc, w, r\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divratios, rcs, w, r = hrpMC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaegte['getHRP'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaegte['getHRP2'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaegte['getIVP'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaegte['risk_parity'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaegte['GMVPortfolio'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaegte['GMVLOPortfolio'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaegte['max_div_port'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist=correlDist(a.corr()) \n",
    "print(dist)\n",
    "link=sch.linkage(dist,'complete')\n",
    "cluster_dict = get_cluster_dict(link)\n",
    "print(cluster_dict)\n",
    "hrp2 = recClusterVar(cluster_dict,link, a.cov())\n",
    "hrp2.plot.bar()\n",
    "mpl.show()\n",
    "sortIx=getQuasiDiag(link) \n",
    "sortIx=a.cov().index[sortIx].tolist() # recover labels \n",
    "hrp=getRecBipart(a.cov(),sortIx)\n",
    "hrp.plot.bar()\n",
    "mpl.show()\n",
    "pd.DataFrame(risk_parity(a.cov())).plot.bar()\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results of Monte Carlo experiment\n",
    "divratios, rcs, w, r = hrpMC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for calculating MDRC\n",
    "def portfolio_risk_deviance(rcs,mean=True):\n",
    "    deviances=[]\n",
    "    for i in rcs:\n",
    "        sam = 0\n",
    "        for j in i:\n",
    "            sam = sam+abs(j-1/len(i))\n",
    "        deviances.append(sam)\n",
    "    if mean:    \n",
    "        return np.array(deviances).mean()\n",
    "    else:\n",
    "        return np.array(deviances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
