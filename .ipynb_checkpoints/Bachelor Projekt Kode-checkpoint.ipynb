{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mpl\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import random, numpy as np, pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "import scipy.cluster\n",
    "import math\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Equally Weighted Portfolio\n",
    "def ewPortfolio(cov,**kargs):\n",
    "    n=len(cov)\n",
    "    return n*[1/n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Risk Parity\n",
    "def calculate_risk_contribution(w,V):\n",
    "    # function that calculates asset contribution to total risk\n",
    "    w = np.matrix(w)\n",
    "    sigma = np.sqrt(calculate_portfolio_var(w,V))\n",
    "    # Marginal Risk Contribution\n",
    "    MRC = np.dot(V,w.T)\n",
    "    # Risk Contribution\n",
    "    RC = np.multiply(MRC,w.T)/sigma\n",
    "    return RC\n",
    "\n",
    "def risk_budget_objective(x,pars):\n",
    "    # calculate portfolio risk\n",
    "    V = pars[0]# covariance table\n",
    "    x_t = pars[1] # risk target in percent of portfolio risk\n",
    "    sig_p =  np.sqrt(calculate_portfolio_var(x,V)) # portfolio sigma\n",
    "    risk_target = np.asmatrix(np.multiply(sig_p,x_t))\n",
    "    asset_RC = calculate_risk_contribution(x,V)\n",
    "    J = sum(np.square(asset_RC-risk_target.T)) # sum of squared error\n",
    "    return J\n",
    "\n",
    "def risk_parity(cov,**kargs):\n",
    "    riskbudget=np.array([1/len(cov)]*len(cov))\n",
    "    w0=np.array([1/len(cov)]*len(cov))\n",
    "    x_t = riskbudget # your risk budget percent of total portfolio risk (equal risk)\n",
    "    cons = ({'type': 'eq', 'fun': total_weight_constraint},\n",
    "    {'type': 'ineq', 'fun': long_only_constraint})\n",
    "    res= minimize(risk_budget_objective, w0, args=[cov,x_t],tol=0.000000000000001, method='SLSQP',constraints=cons) \n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global minimum variance\n",
    "def GMVPortfolio(cov,**kargs):\n",
    "    return np.dot(np.linalg.inv(cov),\n",
    "                  np.ones(len(cov)))/np.dot(np.dot(np.transpose(np.ones(len(cov))),\n",
    "                                                  np.linalg.inv(cov)),np.ones(len(cov)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global minimum variance - long only\n",
    "def GMVLOPortfolio(cov,**kargs):\n",
    "    x0=pd.Series([1/len(cov)]*len(cov))\n",
    "    cons = ({'type': 'eq', 'fun': total_weight_constraint},\n",
    "    {'type': 'ineq', 'fun': long_only_constraint})\n",
    "    res = minimize(calculate_portfolio_var,x0,args=cov,tol=0.000000000000001,method='SLSQP',constraints=cons)\n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse variance portfolio (Risk parity fra De Prado)\n",
    "def getIVP(cov,**kargs):\n",
    "    #Compute the inverse-variance portfolio\n",
    "    ivp=1/np.diag(cov)\n",
    "    ivp/=ivp.sum()\n",
    "    return ivp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximum diversification\n",
    "def calculate_portfolio_var(w,V):\n",
    "    # function that calculates portfolio risk\n",
    "    return (np.dot(np.dot(w,V),w.T))\n",
    "\n",
    "def calc_diversification_ratio(w, V):\n",
    "    # average weighted vol\n",
    "    w_vol = np.dot(np.sqrt(np.diag(V)), w.T)\n",
    "    # portfolio vol\n",
    "    port_vol = np.sqrt(calculate_portfolio_var(w, V))\n",
    "    diversification_ratio = w_vol/port_vol\n",
    "    # return negative for minimization problem (maximize = minimize -)\n",
    "    return -diversification_ratio\n",
    "\n",
    "def total_weight_constraint(x):\n",
    "    return np.sum(x)-1\n",
    "\n",
    "def long_only_constraint(x):\n",
    "    return x\n",
    "\n",
    "def max_div_port(cov,**kargs):\n",
    "    # w0: initial weight\n",
    "    # V: covariance matrix\n",
    "    # bnd: individual position limit\n",
    "    # long only: long only constraint\n",
    "    bnd=None\n",
    "    long_only=True\n",
    "    w0=np.array([1/len(cov)]*len(cov))\n",
    "    cons = ({'type': 'eq', 'fun': total_weight_constraint},)\n",
    "    if long_only: # add in long only constraint\n",
    "        cons = cons + ({'type': 'ineq', 'fun':  long_only_constraint},)\n",
    "    res = minimize(calc_diversification_ratio, w0, bounds=bnd, args=cov, method='SLSQP', constraints=cons)\n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HRP2\n",
    "#This version of HRP divides the weights between clusters\n",
    "\n",
    "import collections\n",
    "def flatten(x):\n",
    "    if isinstance(x, collections.Iterable):\n",
    "        return [a for i in x for a in flatten(i)]\n",
    "    else:\n",
    "        return [x]\n",
    "    \n",
    "def get_cluster_dict(link):\n",
    "    link=np.append(link,np.array([[j] for j in range(int(link[-1,3]),int(link[-1,3])*2-1)]),axis=1)\n",
    "    cluster_dict={}\n",
    "    for i in link[:,0:5].astype(int):\n",
    "        cluster_dict[i[4]]=[]\n",
    "        if i[0]>=link[0,-1]:\n",
    "            cluster_dict[i[4]].append(cluster_dict[i[0]])\n",
    "        else:\n",
    "            cluster_dict[i[4]].append(i[0])\n",
    "        if i[1]>=link[0,-1]:\n",
    "            cluster_dict[i[4]].append(cluster_dict[i[1]])\n",
    "        else:\n",
    "            cluster_dict[i[4]].append(i[1])\n",
    "        \n",
    "    return cluster_dict\n",
    "\n",
    "\n",
    "def recClusterVar(cluster_dict,link, cov):\n",
    "    link=np.append(link,np.array([[j] for j in range(int(link[-1,3]),int(link[-1,3])*2-1)]),axis=1)\n",
    "    w=pd.Series(1,index=[i for i in range(int(link[0,-1]))]) #ændr i til i+1, hvis du skal køre main eller TestStabillity\n",
    "    \n",
    "    for i in reversed(link.astype(int)):\n",
    "        if i[0]>=link[0,-1]:\n",
    "            cluster1 = cluster_dict[i[0]]\n",
    "        else:\n",
    "            cluster1 = i[0]\n",
    "\n",
    "        if i[1]>=link[0,-1]:\n",
    "            cluster2 = cluster_dict[i[1]]\n",
    "        else:\n",
    "            cluster2 = i[1]\n",
    "        \n",
    "        cluster1=[i for i in flatten(cluster1)] #ændr i til i+1, hvis du skal køre main eller TestStabillity\n",
    "        cluster2=[i for i in flatten(cluster2)] #ændr i til i+1, hvis du skal køre main eller TestStabillity\n",
    "        c1_var=getClusterVar(cov,cluster1)\n",
    "        c2_var=getClusterVar(cov,cluster2)\n",
    "        alpha=1-c1_var/(c1_var+c2_var)\n",
    "        w[cluster1]*=alpha # weight 1\n",
    "        w[cluster2]*=1-alpha # weight 2\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HRP\n",
    "def getClusterVar(cov,cItems):\n",
    "    #Compute variance per cluster\n",
    "    cov_=cov.loc[cItems,cItems] # matrix slice\n",
    "    w_=getIVP(cov_).reshape(-1,1)\n",
    "    cVar=np.dot(np.dot(w_.T,cov_),w_)[0,0]\n",
    "    return cVar\n",
    "\n",
    "def getQuasiDiag(link):\n",
    "    # Sort clustered items by distance\n",
    "    link=link.astype(int)\n",
    "    sortIx=pd.Series([link[-1,0],link[-1,1]])\n",
    "    numItems=link[-1,3] #number of original items\n",
    "    while sortIx.max()>=numItems:\n",
    "        sortIx.index=range(0,sortIx.shape[0]*2,2) #make space\n",
    "        df0=sortIx[sortIx>=numItems] # find clusters\n",
    "        i = df0.index;j=df0.values-numItems\n",
    "        sortIx[i]=link[j,0] # item 1\n",
    "        df0=pd.Series(link[j,1],index=i+1)\n",
    "        sortIx=sortIx.append(df0) # item 2\n",
    "        sortIx=sortIx.sort_index() #re-sort\n",
    "        sortIx.index=range(sortIx.shape[0]) # re-index\n",
    "    return sortIx.tolist()\n",
    "\n",
    "def getRecBipart(cov,sortIx):\n",
    "    # Compute HRP alloc\n",
    "    w=pd.Series(1,index=sortIx)\n",
    "    cItems=[sortIx] # initialize all items in one cluster\n",
    "    while len(cItems)>0:\n",
    "        cItems=[i[j:k] for i in cItems for j,k in ((0,len(i)//2),(len(i)//2,len(i))) if len(i)>1] # bi-section\n",
    "        for i in range(0,len(cItems),2):\n",
    "            cItems0=cItems[i] # cluster 1\n",
    "            cItems1=cItems[i+1] # cluster 2\n",
    "            cVar0=getClusterVar(cov,cItems0)\n",
    "            cVar1=getClusterVar(cov,cItems1)\n",
    "            alpha=1-cVar0/(cVar0+cVar1)\n",
    "            w[cItems0]*=alpha # weight 1\n",
    "            w[cItems1]*=1-alpha # weight 2\n",
    "    w.sort_index(inplace=True)\n",
    "    return w\n",
    "    \n",
    "\n",
    "def correlDist(corr):\n",
    "    # A distance matrix based on correlation, where 0<=d[i,j]<=1\n",
    "    #This is a proper diastance metric\n",
    "    dist=((1-corr)/2.)**.5 # distance matrix\n",
    "    return dist\n",
    "\n",
    "\n",
    "def plotCorrMatrix(path,corr,labels=None):\n",
    "    #Heatmap of the correlation matrix\n",
    "    if labels is None: labels=[]\n",
    "    mpl.pcolor(corr)\n",
    "    mpl.colorbar()\n",
    "    mpl.yticks(np.arange(.5,corr.shape[0]+.5),labels)\n",
    "    mpl.xticks(np.arange(.5,corr.shape[0]+.5),labels)\n",
    "    mpl.savefig(path)\n",
    "    mpl.clf();mpl.close()  #reset pylab\n",
    "    return\n",
    "    \n",
    "def generateData(nObs,size0,size1,sigma1,x=np.empty((0,1))):\n",
    "    #Time series of correlated variables\n",
    "    #1)generating some uncorrelated data\n",
    "    np.random.seed(seed=12345);random.seed(12345)\n",
    "    if len(x)==0:\n",
    "        x=np.random.normal(0,1,size=(nObs,size0)) # each row is a variable\n",
    "    #2) creating correlation between the variables\n",
    "    cols=[random.randint(0,size0-1) for i in range(size1)]\n",
    "    q=np.random.normal(0,sigma1,size=(nObs,len(cols)))\n",
    "    y=x[:,cols]+q\n",
    "    x=np.append(x,y,axis=1)\n",
    "    x=pd.DataFrame(x,columns=range(1,x.shape[1]+1))\n",
    "    return x,cols\n",
    "\n",
    "def generateAutocorrelatedData(nObs,correlation,size):\n",
    "    x=np.random.normal(0,1,size=(1,size))\n",
    "    for i in range(nObs-1):\n",
    "        x=np.append(x,correlation*x[i]+np.random.normal(0,1,size=(1,size)),axis=0)\n",
    "    return x\n",
    "\n",
    "def generateCauchyDistData(nObs,size):\n",
    "    x=np.random.standard_cauchy(size=(nObs,size))\n",
    "    return pd.DataFrame(x)\n",
    "\n",
    "def generateT_DistData(nObs,size,df):\n",
    "    x=np.random.standard_t(df,size=(nObs,size))\n",
    "    return pd.DataFrame(x)\n",
    "    \n",
    "def findCorrelatedCols(colnbs,size0):\n",
    "    keys = list(set([i[0] for i in colnbs]))\n",
    "    for i in range(1,size0+1):\n",
    "        if i not in keys:\n",
    "            keys.append(i)     \n",
    "    keys.sort()\n",
    "    clusters={key: [key] for key in keys}\n",
    "    for i in colnbs:\n",
    "        clusters[i[0]].append(i[1])\n",
    "    return clusters\n",
    "\n",
    "def clusterWeights(clusters, hrp):\n",
    "    weights={key:None for key in clusters.keys()}\n",
    "    for i in weights:\n",
    "        weights[i] = sum([hrp.loc[j] for j in clusters[i]])\n",
    "    return list(weights.values())\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    #1) Generate correlated data\n",
    "    nObs, size0,size1,sigma1,correlation = 1000,10,10,0.75,-1\n",
    "    #x = generateAutocorrelatedData(nObs,correlation,size0)\n",
    "    x, cols=generateData(nObs,size0,size1,sigma1)\n",
    "    print(findCorrelatedCols([(j+1,size0+i) for i,j in enumerate(cols,1)],size0))\n",
    "    cov,corr=x.cov(),x.corr()\n",
    "    # 2) compute and plot correl matrix\n",
    "    #corr=pd.DataFrame(np.array([[1,0.7,0.2],[0.7,1,-0.2],[0.2,-0.2,1]]))\n",
    "    plotCorrMatrix('HRP3_corr0.png',corr,labels=corr.columns)\n",
    "    # 3) cluster\n",
    "    dist=correlDist(corr)\n",
    "    link=sch.linkage(dist,'single')\n",
    "    sortIx=getQuasiDiag(link)\n",
    "    sortIx=corr.index[sortIx].tolist() #recover labels\n",
    "    df0=corr.loc[sortIx,sortIx] #re-order\n",
    "    plotCorrMatrix('HRP3_corr1.png',df0,labels=df0.columns)\n",
    "    #4) Capital allocation\n",
    "    hrp=getRecBipart(cov,sortIx)\n",
    "    #5) Capital allocation HRP2\n",
    "    cluster_dict = get_cluster_dict(link)\n",
    "    hrp2 = recClusterVar(cluster_dict,link, cov)\n",
    "    plot_weights(hrp, hrp2, cov, x)\n",
    "    scipy.cluster.hierarchy.dendrogram(link, labels=[i+1 for i in range(size0+size1)])\n",
    "    \n",
    "    return hrp\n",
    "\n",
    "\n",
    "def testStability():\n",
    "    nObs, size0,size1,sigma1,recalc_time, samplesize,correlation = 528,5,5,0.5,22,264,0\n",
    "    #x = generateAutocorrelatedData(nObs,correlation,size0)\n",
    "    x,cols=generateData(nObs,size0,size1,sigma1)\n",
    "    clusters=findCorrelatedCols([(j+1,size0+i) for i,j in enumerate(cols,1)],size0)\n",
    "    clusterweights=[]\n",
    "    weights=[]\n",
    "    weights2=[]\n",
    "    print(clusters)\n",
    "    for i in range(int((nObs-samplesize)/recalc_time)+1):\n",
    "        x_sample = x.iloc[i*recalc_time:samplesize+recalc_time*i]\n",
    "        cov,corr=x_sample.cov(),x_sample.corr()\n",
    "        dist=correlDist(corr)\n",
    "        link=sch.linkage(dist,'single')\n",
    "        sortIx=getQuasiDiag(link)\n",
    "        sortIx=corr.index[sortIx].tolist() #recover labels\n",
    "        df0=corr.loc[sortIx,sortIx] #re-order\n",
    "        hrp=getRecBipart(cov,sortIx)\n",
    "        cluster_dict = get_cluster_dict(link)\n",
    "        hrp2 = recClusterVar(cluster_dict,link, cov)\n",
    "        #plotCorrMatrix('HRP3_corr{}.png'.format(i),df0,labels=df0.columns)\n",
    "        clusterweights.append(clusterWeights(clusters,hrp))\n",
    "        weights.append(list(hrp.sort_index().values))\n",
    "        weights2.append(hrp2.values)\n",
    "    return pd.DataFrame(weights), pd.DataFrame(weights2)\n",
    "    return pd.DataFrame(clusterweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_contribution(weights,cov):\n",
    "    portvar = np.dot(np.dot(weights,cov),weights.T)\n",
    "    rc=[(weights[i]*np.dot(cov,weights)[i])/portvar for i in range(len(weights))]\n",
    "    return rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Barplots of allocations\n",
    "def plot_weights(hrp, hrp2, cov, data):\n",
    "    index = list(data.columns)\n",
    "    \n",
    "    #HRP\n",
    "    hrp = hrp.sort_values(ascending=False)\n",
    "    hrp.plot.bar(figsize = (15,7))\n",
    "    mpl.title(\"HRP\")\n",
    "    mpl.ylabel(\"Weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((0, 0.5))\n",
    "    mpl.show()\n",
    "    \n",
    "    #HRP2\n",
    "    hrp2 = hrp2.sort_values(ascending=False)\n",
    "    hrp2.plot.bar(figsize = (15,7))\n",
    "    mpl.title(\"HRP2\")\n",
    "    mpl.ylabel(\"Weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((0, 0.5))\n",
    "    mpl.show()\n",
    "\n",
    "    #Naive Risk-Parity\n",
    "    ivp = getIVP(cov)\n",
    "    ivp = pd.Series(ivp, index=index)\n",
    "    ivp = ivp.sort_values(ascending=False)\n",
    "    ivp.plot.bar(figsize = (15,7))\n",
    "    mpl.title(\"Naive Risk parity (IVP)\")\n",
    "    mpl.ylabel(\"Weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((0,0.5))\n",
    "    mpl.show()\n",
    "    \n",
    "    #Risk Parity\n",
    "    rp = risk_parity(cov)\n",
    "    rp = pd.Series(rp, index=index)\n",
    "    rp = rp.sort_values(ascending=False)\n",
    "    rp.plot.bar(figsize = (15,7))\n",
    "    mpl.title(\"Risk Parity\")\n",
    "    mpl.ylabel(\"Weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((0, 0.5))\n",
    "    mpl.show()\n",
    "    \n",
    "    #GMV\n",
    "    gmv = GMVPortfolio(cov)\n",
    "    gmv = pd.Series(gmv, index=index)\n",
    "    gmv = gmv.sort_values(ascending=False)\n",
    "    gmv.plot.bar(figsize = (15,7))\n",
    "    mpl.title(\"GMV\")\n",
    "    mpl.ylabel(\"Weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((-0.2, 0.5))\n",
    "    mpl.show()\n",
    "    \n",
    "    #GMV Long-only\n",
    "    gmvlo = GMVLOPortfolio(cov)\n",
    "    gmvlo = pd.Series(gmvlo, index=index)\n",
    "    gmvlo = gmvlo.sort_values(ascending = False)\n",
    "    gmvlo.plot.bar(figsize=(15,7))\n",
    "    mpl.title(\"GMV Long-only\")\n",
    "    mpl.ylabel(\"weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((0,0.5))\n",
    "    mpl.show()\n",
    "    \n",
    "    \n",
    "    #Maximum_Div_port\n",
    "    mdv = pd.Series(max_div_port(cov), index=index)\n",
    "    mdv = mdv.sort_values(ascending=False)\n",
    "    mdv.plot.bar(figsize = (15,7))\n",
    "    mpl.title(\"Maximum Diversification Portfolio\")\n",
    "    mpl.ylabel(\"Weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((-0.1, 0.5))\n",
    "    mpl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#data fra excel fil arp_strategies\n",
    "for i in range(5):\n",
    "    arp_data = pd.read_excel(\"arp_strategies.xlsx\", sheet_name=\"S_{}\".format(i+1))\n",
    "    arp_data = arp_data.iloc[2611:] #nogle strategier starter først fra 2005, så de første 2611 rækker fjernes\n",
    "    arp_data = arp_data.drop(['Date'], axis=1)\n",
    "    if i == 0:\n",
    "        arp_data_samlet = arp_data\n",
    "    else:\n",
    "        arp_data_samlet = pd.merge(arp_data_samlet,arp_data,right_index = True, left_index = True)\n",
    "\n",
    "arp_data = arp_data_samlet    \n",
    "del arp_data_samlet\n",
    "arp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data fra excel fil Betting against beta\n",
    "#den seneste faktor data starter d. 7/3/1989, som er forskellige rækker i nogle af datasættet\n",
    "dates = pd.read_excel('Betting Against Beta Equity Factors Daily.xlsx','MKT',skiprows=17020,usecols='A', header = 0)\n",
    "dates.columns=['Date']\n",
    "dates=pd.Series(dates['Date'])\n",
    "dd= pd.to_datetime(dates)\n",
    "\n",
    "risk_free_rates = pd.read_excel('Betting Against Beta Equity Factors Daily.xlsx','RF',skiprows=17588, usecols='A')\n",
    "\n",
    "sheet_names=['MKT','SMB','HML FF','HML Devil']\n",
    "for i in sheet_names:\n",
    "    faktor_data = pd.read_excel('Betting Against Beta Equity Factors Daily.xlsx',i, skiprows=17021, usecols='AC')\n",
    "    if i == 'MKT':\n",
    "        data_samlet = faktor_data\n",
    "    else:\n",
    "        data_samlet = pd.merge(data_samlet,faktor_data,right_index = True, left_index = True)\n",
    "\n",
    "#Henter UMD\n",
    "umd_data = pd.read_excel('Betting Against Beta Equity Factors Daily.xlsx','UMD', skiprows=16871, usecols='AC')\n",
    "data_samlet = pd.merge(data_samlet,umd_data,right_index = True, left_index = True)\n",
    "\n",
    "#Henter BAB\n",
    "bab_data = pd.read_excel('Betting Against Beta Equity Factors Daily.xlsx','BAB Factors', skiprows=15712, usecols='AC')\n",
    "data_samlet = pd.merge(data_samlet,bab_data,right_index = True, left_index = True)\n",
    "\n",
    "#Henter QMJ\n",
    "qmj_data = pd.read_excel('Quality Minus Junk Factors Daily.xlsx','QMJ Factors', skiprows=8112, usecols='AC')\n",
    "data_samlet = pd.merge(data_samlet,qmj_data,right_index = True, left_index = True)\n",
    "\n",
    "sheet_names=['MKT','SMB','HML FF','HML Devil', 'UMD', 'BAB Factors','QMJ Factors']\n",
    "faktor_data = data_samlet        \n",
    "faktor_data.columns = sheet_names\n",
    "faktor_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.read_excel('Betting Against Beta Equity Factors Daily.xlsx','MKT',skiprows=18434,usecols='A', header = 0)\n",
    "dates.columns=['Date']\n",
    "dates=pd.Series(dates['Date'])\n",
    "dd= pd.to_datetime(dates[520:]).reset_index(drop=True)\n",
    "\n",
    "risk_free_rates = pd.read_excel('Betting Against Beta Equity Factors Daily.xlsx','RF',skiprows=19001, usecols='B')\n",
    "risk_free_rates = (risk_free_rates[520:-18])\n",
    "#Henter alle lande i MKT\n",
    "mkt_data = pd.read_excel('Betting Against Beta Equity Factors Daily.xlsx','MKT', skiprows=18434, usecols='B:Y')\n",
    "mkt_global = pd.read_excel('Betting Against Beta Equity Factors Daily.xlsx','MKT', skiprows=18434, usecols='Z', header=0) #Global som benchmark?\n",
    "mkt_data.columns = ['AUS', 'AUT', 'BEL', 'CAN', 'CHE', 'DEU', 'DNK', 'ESP', 'FIN', 'FRA', 'GBR', 'GRC', 'HKG', 'IRL', 'ISR', 'ITA', 'JPN', 'NLD', 'NOR', 'NZL', 'PRT', 'SGP', 'SWE', 'USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arp_strategies.xlsx_strategies.xlsx_data = mkt_data\n",
    "arp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "arp_data = mkt_data\n",
    "arp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Henter data.csv\n",
    "data2 = pd.read_csv('data.csv')\n",
    "data2.columns\n",
    "data2.drop(['gvkey', 'iid', 'datadate','cusip','conm','cheqv','divd','divsp','ajexdi','cshoc','cshtrd','prccd','prchd','prcod','trfd','ggroup','gind','gsector','gsubind','naics','sic'], axis=1, inplace=True)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_2():\n",
    "    #1) data\n",
    "    x = arp_data\n",
    "    cov,corr=x.cov(),x.corr()\n",
    "    # 2) compute and plot correl matrix\n",
    "    #corr=pd.DataFrame(np.array([[1,0.7,0.2],[0.7,1,-0.2],[0.2,-0.2,1]]))\n",
    "    plotCorrMatrix('HRP3_corr0.png',corr,labels=corr.columns)\n",
    "    # 3) cluster\n",
    "    dist=correlDist(corr)\n",
    "    link=sch.linkage(dist,'median')\n",
    "    sortIx=getQuasiDiag(link)\n",
    "    sortIx=corr.index[sortIx].tolist() #recover labels\n",
    "    df0=corr.loc[sortIx,sortIx] #re-order\n",
    "    plotCorrMatrix('HRP3_corr1.png',df0,labels=df0.columns)\n",
    "    #4) Capital allocation\n",
    "    hrp=getRecBipart(cov,sortIx)\n",
    "    \n",
    "    cluster_dict = get_cluster_dict(link)\n",
    "    hrp2 = recClusterVar(cluster_dict,link, cov)\n",
    "    \n",
    "    plot_weights(hrp, hrp2, cov, x)\n",
    "    mpl.figure(figsize=[20,10])\n",
    "    scipy.cluster.hierarchy.dendrogram(link)#, labels=sortIx)\n",
    "    \n",
    "    \n",
    "    return hrp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main_2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_3():\n",
    "    #Formålet er at se hvordan vægtene i porteføljen udvikler sig over tid.\n",
    "    re_calc_time,sample_size = 22,520\n",
    "    x = pd.DataFrame(arp_data)\n",
    "    #Nye\n",
    "    HRP_portfolio_return=[]\n",
    "    RP_portfolio_return=[]\n",
    "    RiskP_portfolio_return=[]\n",
    "    GMV_portfolio_return=[]\n",
    "    GMVLO_portfolio_return=[]\n",
    "    MD_portfolio_return=[]\n",
    "    EW_portfolio_return=[]\n",
    "    HRP2_portfolio_return=[]\n",
    "    \n",
    "    hrp_weights=[]\n",
    "    hrp2_weights=[]\n",
    "    gmv_weights=[]\n",
    "    mdv_weights=[]\n",
    "    ivp_weights=[]\n",
    "    rp_weights=[]\n",
    "    gmvlo_weights=[]\n",
    "    \n",
    "    realisedRC_RP = []\n",
    "    realisedRC_HRP = []\n",
    "    realisedRC_HRP2 = []\n",
    "    realisedRC_GMV = []\n",
    "    realisedRC_MD = []\n",
    "    realisedRC_IVP = []\n",
    "    realisedRC_GMVLO = []\n",
    "    realisedRC_EW = []\n",
    "    \n",
    "    realisedDR_RP = []\n",
    "    realisedDR_HRP = []\n",
    "    realisedDR_HRP2 = []\n",
    "    realisedDR_GMV = []\n",
    "    realisedDR_MD = []\n",
    "    realisedDR_IVP = []\n",
    "    realisedDR_GMVLO = []\n",
    "    realisedDR_EW = []\n",
    "    \n",
    "    EW_weight=[1/len(arp_data.columns)]*len(arp_data.columns)\n",
    "    returns=[]\n",
    "    pointers = range(520,len(x)-22,re_calc_time)\n",
    "    for pointer in pointers:\n",
    "        print(\"pointer= {}\".format(pointer))\n",
    "        #Henter data\n",
    "        x_sample = x.iloc[pointer-sample_size:pointer] \n",
    "        cov,corr=x_sample.cov().reset_index(drop=True) ,x_sample.corr().reset_index(drop=True)\n",
    "        cov.columns, corr.columns = [i for i in range(len(cov))], [i for i in range(len(cov))]\n",
    "        #HRP\n",
    "        dist=correlDist(corr)\n",
    "        link=sch.linkage(dist,'complete')\n",
    "        sortIx=getQuasiDiag(link)\n",
    "        sortIx=corr.index[sortIx].tolist() #recover labels\n",
    "        #df0=corr.loc[sortIx,sortIx]\n",
    "        #plotCorrMatrix('HRP3_corr{}.png'.format(i),df0,labels=df0.columns)\n",
    "        hrp=getRecBipart(cov,sortIx).sort_index()\n",
    "        #HRP2\n",
    "        cluster_dict = get_cluster_dict(link)\n",
    "        hrp2 = recClusterVar(cluster_dict,link, cov).sort_index()\n",
    "        #IVP\n",
    "        ivp = getIVP(cov)\n",
    "        #GMV\n",
    "        gmv = GMVPortfolio(cov)\n",
    "        #GMV Long-only\n",
    "        gmvlo = GMVLOPortfolio(cov)\n",
    "        #MD\n",
    "        mdv = pd.Series(max_div_port(cov))\n",
    "        #Risk Parity\n",
    "        rp = pd.Series(risk_parity(cov))\n",
    "        hrp_weights.append(hrp)\n",
    "        hrp2_weights.append(hrp2)\n",
    "        gmv_weights.append(gmv)\n",
    "        gmvlo_weights.append(gmvlo)\n",
    "        mdv_weights.append(mdv)\n",
    "        ivp_weights.append(ivp)\n",
    "        rp_weights.append(rp)\n",
    "        \n",
    "        #realised risk contribution\n",
    "        cov_out = x.iloc[pointer:pointer+re_calc_time].cov()\n",
    "        realisedRC_RP.append(calculate_risk_contribution(rp, cov_out)/np.sqrt(calculate_portfolio_var(rp,cov_out)))\n",
    "        realisedRC_HRP.append(calculate_risk_contribution(hrp, cov_out)/np.sqrt(calculate_portfolio_var(hrp,cov_out)))\n",
    "        realisedRC_HRP2.append(calculate_risk_contribution(hrp2, cov_out)/np.sqrt(calculate_portfolio_var(hrp2,cov_out)))\n",
    "        realisedRC_GMV.append(calculate_risk_contribution(gmv, cov_out)/np.sqrt(calculate_portfolio_var(gmv,cov_out)))\n",
    "        realisedRC_MD.append(calculate_risk_contribution(mdv, cov_out)/np.sqrt(calculate_portfolio_var(mdv,cov_out)))\n",
    "        realisedRC_IVP.append(calculate_risk_contribution(ivp, cov_out)/np.sqrt(calculate_portfolio_var(ivp,cov_out)))\n",
    "        realisedRC_GMVLO.append(calculate_risk_contribution(gmvlo, cov_out)/np.sqrt(calculate_portfolio_var(gmvlo,cov_out)))\n",
    "        realisedRC_EW.append(calculate_risk_contribution(EW_weight, cov_out)/np.sqrt(calculate_portfolio_var(pd.Series(EW_weight),cov_out)))\n",
    "        \n",
    "        \n",
    "        #realised diversification ratio\n",
    "        realisedDR_RP.append(calc_diversification_ratio(rp, cov_out))\n",
    "        realisedDR_HRP.append(calc_diversification_ratio(hrp, cov_out))\n",
    "        realisedDR_HRP2.append(calc_diversification_ratio(hrp2, cov_out))\n",
    "        realisedDR_GMV.append(calc_diversification_ratio(gmv, cov_out))\n",
    "        realisedDR_MD.append(calc_diversification_ratio(mdv, cov_out))\n",
    "        realisedDR_IVP.append(calc_diversification_ratio(ivp, cov_out))\n",
    "        realisedDR_GMVLO.append(calc_diversification_ratio(gmvlo, cov_out))\n",
    "        realisedDR_EW.append(calc_diversification_ratio(pd.Series(EW_weight), cov_out))\n",
    "        \n",
    "        for j in range(re_calc_time):\n",
    "            HRP_portfolio_return.append(np.dot(hrp,arp_data.iloc[pointer+j]))\n",
    "            RP_portfolio_return.append(np.dot(ivp,arp_data.iloc[pointer+j]))\n",
    "            GMV_portfolio_return.append(np.dot(gmv,arp_data.iloc[pointer+j]))\n",
    "            GMVLO_portfolio_return.append(np.dot(gmvlo,arp_data.iloc[pointer+j]))\n",
    "            MD_portfolio_return.append(np.dot(mdv,arp_data.iloc[pointer+j]))\n",
    "            EW_portfolio_return.append(np.dot(EW_weight,arp_data.iloc[pointer+j]))\n",
    "            HRP2_portfolio_return.append(np.dot(hrp2,arp_data.iloc[pointer+j]))\n",
    "            RiskP_portfolio_return.append(np.dot(rp,arp_data.iloc[pointer+j]))\n",
    "    return pd.DataFrame(HRP_portfolio_return), pd.DataFrame(RP_portfolio_return), pd.DataFrame(GMV_portfolio_return), pd.DataFrame(GMVLO_portfolio_return), pd.DataFrame(MD_portfolio_return),pd.DataFrame(EW_portfolio_return), pd.DataFrame(HRP2_portfolio_return), pd.DataFrame(RiskP_portfolio_return), pd.DataFrame(returns),pd.DataFrame(hrp_weights),pd.DataFrame(hrp2_weights),pd.DataFrame(gmv_weights),pd.DataFrame(mdv_weights),pd.DataFrame(ivp_weights), pd.DataFrame(rp_weights), pd.DataFrame(gmvlo_weights), realisedRC_RP, realisedRC_HRP, realisedRC_HRP2, realisedRC_GMV, realisedRC_GMVLO, realisedRC_IVP, realisedRC_MD, realisedRC_EW, realisedDR_GMV,realisedDR_GMVLO,realisedDR_HRP,realisedDR_HRP2,realisedDR_IVP,realisedDR_MD,realisedDR_RP,realisedDR_EW\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_at_risk(returns, confidence_level=.05):\n",
    "    return returns.quantile(confidence_level, interpolation='higher')\n",
    "\n",
    "\n",
    "def expected_shortfall(returns, confidence_level=.05):\n",
    "    var = value_at_risk(returns, confidence_level)\n",
    "\n",
    "    return returns[returns.lt(var)].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HRP_weights,RP_weights,GMV_weights,GMVLO_weights,MD_weights,EW_weights,HRP2_weights, RiskP_weights, returns, hrp_weights, hrp2_weights, gmv_weights, mdv_weights, ivp_weights, rp_weights, gmvlo_weights, realisedRC_RP, realisedRC_HRP, realisedRC_HRP2, realisedRC_GMV, realisedRC_GMVLO, realisedRC_IVP, realisedRC_MD, realisedRC_EW, realisedDR_GMV, realisedDR_GMVLO, realisedDR_HRP, realisedDR_HRP2, realisedDR_IVP, realisedDR_MD, realisedDR_RP, realisedDR_EW = main_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HRP_weighted_returns = (HRP_weights*returns)\n",
    "HRP_portfolio_returns = HRP_weights\n",
    "HRP_cumulative_returns = (HRP_portfolio_returns + 1).cumprod()\n",
    "HRP_cumulative_returns['dates']=dd\n",
    "HRP_cumulative_returns.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "\n",
    "#HRP2\n",
    "HRP2_portfolio_returns = HRP2_weights\n",
    "HRP2_cumulative_returns = (HRP2_portfolio_returns +1).cumprod()\n",
    "HRP2_cumulative_returns['dates']=dd\n",
    "HRP2_cumulative_returns.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "#RP_weighted_returns = (RP_weights*returns)\n",
    "RP_portfolio_returns = RP_weights\n",
    "RP_cumulative_returns = (RP_portfolio_returns + 1).cumprod()\n",
    "RP_cumulative_returns['dates']=dd\n",
    "RP_cumulative_returns.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "#RiskP\n",
    "RiskP_portfolio_returns = RiskP_weights\n",
    "RiskP_cumulative_returns = (RiskP_portfolio_returns + 1).cumprod()\n",
    "RiskP_cumulative_returns['dates']=dd\n",
    "RiskP_cumulative_returns.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "#GMV_weighted_returns = (GMV_weights*returns)\n",
    "GMV_portfolio_returns = GMV_weights\n",
    "GMV_cumulative_returns = (GMV_portfolio_returns + 1).cumprod()\n",
    "GMV_cumulative_returns['dates']=dd\n",
    "GMV_cumulative_returns.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "#GMVLO_weighted_returns = (GMV_weights*returns)\n",
    "GMVLO_portfolio_returns = GMVLO_weights\n",
    "GMVLO_cumulative_returns = (GMVLO_portfolio_returns + 1).cumprod()\n",
    "GMVLO_cumulative_returns['dates']=dd\n",
    "GMVLO_cumulative_returns.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "#MD_weighted_returns = (MD_weights*returns)\n",
    "MD_portfolio_returns = MD_weights\n",
    "MD_cumulative_returns = (MD_portfolio_returns + 1).cumprod()\n",
    "MD_cumulative_returns['dates']=dd\n",
    "MD_cumulative_returns.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "#EW_weighted_returns = (EW_weights*returns)\n",
    "EW_portfolio_returns = EW_weights\n",
    "EW_cumulative_returns = (EW_portfolio_returns + 1).cumprod()\n",
    "EW_cumulative_returns['dates']=dd\n",
    "EW_cumulative_returns.set_index('dates',inplace=True,drop=True)\n",
    "\n",
    "\n",
    "#global\n",
    "mkt_global_cumulative_returns = (mkt_global[520:] + 1).cumprod()\n",
    "mkt_global_cumulative_returns =mkt_global_cumulative_returns.reset_index(drop=True)\n",
    "mkt_global_cumulative_returns['dates']=dd\n",
    "mkt_global_cumulative_returns.set_index('dates',inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = HRP_cumulative_returns.plot(figsize=[15,10])\n",
    "RP_cumulative_returns.plot(figsize=[15,10], ax=ax)\n",
    "RiskP_cumulative_returns.plot(figsize=[15,10],ax=ax)\n",
    "GMV_cumulative_returns.plot(figsize=[15,10], ax=ax)\n",
    "GMVLO_cumulative_returns.plot(figsize=[15,10], ax=ax)\n",
    "MD_cumulative_returns.plot(figsize=[15,10], ax=ax)\n",
    "EW_cumulative_returns.plot(figsize=[15,10], ax=ax)\n",
    "HRP2_cumulative_returns.plot(figsize=[15,10], ax=ax)\n",
    "\n",
    "mkt_global_cumulative_returns.plot(figsize=[15,10], ax=ax)\n",
    "\n",
    "mpl.legend(['HRP', 'Naive RP','RP', 'GMV','GMVLO', 'MD','EW','HRP2', 'Global'])\n",
    "mpl.ylabel(\"Accumulated returns (Index 1)\")\n",
    "mpl.show()\n",
    "#Hvordan får vi de rigtige datoer på x akse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########returns for sharpe-ratio\n",
    "risk_free_rates = risk_free_rates.reset_index(drop=True)\n",
    "sr_HRP=pd.Series(((np.array(HRP_portfolio_returns)-np.array(risk_free_rates))+1).cumprod())\n",
    "sr_HRP2=pd.Series(((np.array(HRP2_portfolio_returns)-np.array(risk_free_rates))+1).cumprod())\n",
    "sr_RP=pd.Series(((np.array(RP_portfolio_returns)-np.array(risk_free_rates))+1).cumprod())\n",
    "sr_RiskP=pd.Series(((np.array(RiskP_portfolio_returns)-np.array(risk_free_rates))+1).cumprod())\n",
    "sr_GMV=pd.Series(((np.array(GMV_portfolio_returns)-np.array(risk_free_rates))+1).cumprod())\n",
    "sr_GMVLO=pd.Series(((np.array(GMVLO_portfolio_returns)-np.array(risk_free_rates))+1).cumprod())\n",
    "sr_MD=pd.Series(((np.array(MD_portfolio_returns)-np.array(risk_free_rates))+1).cumprod())\n",
    "sr_EW=pd.Series(((np.array(EW_portfolio_returns)-np.array(risk_free_rates))+1).cumprod())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Danner tabel der sammenligner allokerings strategierne\n",
    "l=len(HRP_cumulative_returns)-1\n",
    "allo_comp = [[(HRP_cumulative_returns[0][-1])**(1/24)-1, (HRP2_cumulative_returns[0][-1])**(1/24)-1, (RP_cumulative_returns[0][-1])**(1/24)-1, (GMV_cumulative_returns[0][-1])**(1/24)-1, (GMVLO_cumulative_returns[0][-1])**(1/24)-1, (MD_cumulative_returns[0][-1])**(1/24)-1,(RiskP_cumulative_returns[0][-1])**(1/24)-1 ,(EW_cumulative_returns[0][-1])**(1/24)-1]]\n",
    "allo_comp.append([HRP_portfolio_returns.std()[0]*np.sqrt(250), HRP2_portfolio_returns.std()[0]*np.sqrt(250), RP_portfolio_returns.std()[0]*np.sqrt(250), GMV_portfolio_returns.std()[0]*np.sqrt(250), GMVLO_portfolio_returns.std()[0]*np.sqrt(250),MD_portfolio_returns.std()[0]*np.sqrt(250),RiskP_portfolio_returns.std()[0]*np.sqrt(250),EW_portfolio_returns.std()[0]*np.sqrt(250)])\n",
    "allo_comp.append([expected_shortfall(HRP_portfolio_returns)[0], expected_shortfall(HRP2_portfolio_returns)[0], expected_shortfall(RP_portfolio_returns)[0], expected_shortfall(GMV_portfolio_returns)[0], expected_shortfall(GMVLO_portfolio_returns)[0], expected_shortfall(MD_portfolio_returns)[0],expected_shortfall(RiskP_portfolio_returns)[0], expected_shortfall(EW_portfolio_returns)[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allo_comp=pd.DataFrame(allo_comp, columns=['HRP', 'HRP2','Naive RP','GMV','GMVLO','MD','Risk Parity','EW'],index=['Annualized return','Annualized standard deviance','Expected shortfall'])\n",
    "#Tilføjer Sharpe Ratio til tabellen\n",
    "allo_comp.loc['Sharpe Ratio']=allo_comp.loc['Annualized return']/allo_comp.loc['Annualized standard deviance']\n",
    "#Expected shortfall 14 dage\n",
    "N = 14\n",
    "\n",
    "HRP_portfolio_returns14 = HRP_portfolio_returns.groupby(HRP_portfolio_returns.index // N).sum()\n",
    "HRP_portfolio_returns.groupby(HRP_portfolio_returns.index // N).sum()\n",
    "HRP2_portfolio_returns14 = HRP_portfolio_returns.groupby(HRP_portfolio_returns.index // N).sum()\n",
    "RP_portfolio_returns14 = RP_portfolio_returns.groupby(RP_portfolio_returns.index // N).sum()\n",
    "GMV_portfolio_returns14 = GMV_portfolio_returns.groupby(GMV_portfolio_returns.index // N).sum()\n",
    "GMVLO_portfolio_returns14 = GMVLO_portfolio_returns.groupby(GMVLO_portfolio_returns.index // N).sum()\n",
    "MD_portfolio_returns14 = MD_portfolio_returns.groupby(MD_portfolio_returns.index // N).sum()\n",
    "RiskP_portfolio_returns14 = RiskP_portfolio_returns.groupby(RiskP_portfolio_returns.index // N).sum()\n",
    "EW_portfolio_returns14 = EW_portfolio_returns.groupby(EW_portfolio_returns.index // N).sum()\n",
    "allo_comp.loc['Expected shortfall 14 days'] = [expected_shortfall(HRP_portfolio_returns14)[0], expected_shortfall(HRP2_portfolio_returns14)[0], expected_shortfall(RP_portfolio_returns14)[0], expected_shortfall(GMV_portfolio_returns14)[0], expected_shortfall(GMVLO_portfolio_returns14)[0], expected_shortfall(MD_portfolio_returns14)[0], expected_shortfall(RiskP_portfolio_returns14)[0],expected_shortfall(EW_portfolio_returns14)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tilføjer realised risk contribution og diversification ratio til tabellen\n",
    "allo_comp.loc['Realised deviance risk contribution'] = [portfolio_risk_deviance(realisedRC_HRP), portfolio_risk_deviance(realisedRC_HRP2), portfolio_risk_deviance(realisedRC_IVP), portfolio_risk_deviance(realisedRC_GMV), portfolio_risk_deviance(realisedRC_GMVLO), portfolio_risk_deviance(realisedRC_MD), portfolio_risk_deviance(realisedRC_RP), portfolio_risk_deviance(realisedRC_EW)]\n",
    "allo_comp.loc['Realised diversification ratio'] = [abs(statistics.mean(realisedDR_HRP)), abs(statistics.mean(realisedDR_HRP2)), abs(statistics.mean(realisedDR_IVP)), abs(statistics.mean(realisedDR_GMV)), abs(statistics.mean(realisedDR_GMVLO)), abs(statistics.mean(realisedDR_MD)), abs(statistics.mean(realisedDR_RP)), abs(statistics.mean(realisedDR_EW))]\n",
    "allo_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HRP_weights.columns = arp_data.columns\n",
    "weight_stats=pd.DataFrame()\n",
    "weight_stats['mean'] = HRP_weights.mean()\n",
    "weight_stats['std.dev'] = HRP_weights.std()\n",
    "weight_stats['variance'] = HRP_weights.var()\n",
    "weight_stats.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rene_strategier=(arp_data.iloc[262:]+1).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rene_strategier.plot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmv_weights.plot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrp2_weights.plot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrp_weights.plot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdv_weights.plot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rp_weights.plot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmvlo_weights.plot(figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Monte Carlo :-)\n",
    "import time\n",
    "\n",
    "def generateData(nObs,sLength,size0,size1,mu0,sigma0,sigma1F):\n",
    "    # Time series of correlated variables\n",
    "    #1) generate random uncorrelated data\n",
    "    x=np.random.normal(mu0,sigma0,size=(nObs,size0)) # each row is a variable \n",
    "    #2) create correlation between the variables\n",
    "    cols=[random.randint(0,size0-1) for i in range(size1)]\n",
    "    y=x[:,cols]+np.random.normal(0,sigma0*sigma1F,size=(nObs,len(cols)))\n",
    "    x=np.append(x,y,axis=1)\n",
    "    #3) add common random shock\n",
    "    point=np.random.randint(sLength,nObs-1,size=2)\n",
    "    x[np.ix_(point,[cols[0],size0])]=np.array([[-.5,-.5],[2,2]])\n",
    "    #4) add specific random shock\n",
    "    point=np.random.randint(sLength,nObs-1,size=2)\n",
    "    x[point,cols[-1]]=np.array([-.5,2])\n",
    "    return x,cols\n",
    "\n",
    "def getHRP(cov,corr):\n",
    "    # Construct a hierarchical portfolio\n",
    "    corr,cov=pd.DataFrame(corr),pd.DataFrame(cov)\n",
    "    dist=correlDist(corr) \n",
    "    link=sch.linkage(dist,'single')\n",
    "    sortIx=getQuasiDiag(link) \n",
    "    sortIx=corr.index[sortIx].tolist() # recover labels \n",
    "    hrp=getRecBipart(cov,sortIx)\n",
    "    return hrp.sort_index()\n",
    "\n",
    "def getHRP2(cov,corr):\n",
    "    # Construct a hierarchical portfolio\n",
    "    corr,cov=pd.DataFrame(corr),pd.DataFrame(cov)\n",
    "    dist=correlDist(corr) \n",
    "    link=sch.linkage(dist,'single')\n",
    "    cluster_dict = get_cluster_dict(link)\n",
    "    hrp2 = recClusterVar(cluster_dict,link, cov)\n",
    "    return hrp2\n",
    "\n",
    "\n",
    "def hrpMC(numIters=10000,nObs=520,size0=5,size1=5,mu0=0,sigma0=0.01, sigma1F=.25,sLength=260,rebal=22):\n",
    "    start_time = time.time()\n",
    "    # Monte Carlo experiment on HRP\n",
    "    methods=[getHRP,getHRP2,getIVP,risk_parity,GMVPortfolio,GMVLOPortfolio,max_div_port,ewPortfolio]#,getCLA] \n",
    "    stats,numIter={i.__name__:pd.Series() for i in methods},0\n",
    "    pointers=range(sLength,nObs,rebal)\n",
    "    divratio={i.__name__:pd.Series() for i in methods}\n",
    "    rc={i.__name__:pd.Series() for i in methods}\n",
    "    #w={i.__name__:pd.DataFrame(columns=[i for i in range(size0+size1)]) for i in methods}\n",
    "    while numIter<numIters:\n",
    "        print (numIter)\n",
    "        #1) Prepare data for one experiment \n",
    "        x,cols=generateData(nObs,sLength,size0,size1,mu0,sigma0,sigma1F)\n",
    "        r={i.__name__:pd.Series() for i in methods}\n",
    "        #2) Compute portfolios in-sample\n",
    "        for pointer in pointers:\n",
    "            x_=x[pointer-sLength:pointer]\n",
    "            cov_,corr_=np.cov(x_,rowvar=0),np.corrcoef(x_,rowvar=0) \n",
    "            #3) Compute performance out-of-sample\n",
    "            x_=x[pointer:pointer+rebal]\n",
    "            cov_out=pd.DataFrame(np.cov(x[pointer:pointer+rebal],rowvar=0))\n",
    "            for func in methods:\n",
    "                w_=pd.Series(func(cov=cov_,corr=corr_))\n",
    "                # callback \n",
    "                r_=pd.Series(np.dot(x_,w_))\n",
    "                divratio_ = calc_diversification_ratio(w_,cov_out)\n",
    "                divratio[func.__name__]=divratio[func.__name__].append(pd.Series(divratio_))\n",
    "                rc_ = np.squeeze(np.asarray(calculate_risk_contribution(w_, cov_out)))/np.sqrt(calculate_portfolio_var(w_,cov_out))\n",
    "                rc[func.__name__]=rc[func.__name__].append(pd.Series(rc_))\n",
    "                r[func.__name__]=r[func.__name__].append(r_)\n",
    "        #4) Evaluate and store results\n",
    "        for func in methods:\n",
    "            r_=r[func.__name__].reset_index(drop=True)\n",
    "            p_=(1+r_).cumprod()\n",
    "            stats[func.__name__].loc[numIter]=p_.iloc[-1]-1 # terminal return\n",
    "        print(\"Vi har nu kørt i\", time.time() - start_time, \"sekunder\")\n",
    "        numIter+=1\n",
    "    #5) Report results\n",
    "    stats=pd.DataFrame.from_dict(stats,orient='columns')\n",
    "    stats.to_csv('stats.csv')\n",
    "    df0,df1=stats.std(),stats.var()\n",
    "    print(pd.concat([df0,df1,df1/df1['getHRP']-1],axis=1))\n",
    "    return divratio,rc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divratios, rcs = hrpMC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaegte['getHRP'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaegte['getHRP2'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaegte['getIVP'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaegte['risk_parity'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaegte['GMVPortfolio'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaegte['GMVLOPortfolio'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaegte['max_div_port'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist=correlDist(a.corr()) \n",
    "print(dist)\n",
    "link=sch.linkage(dist,'complete')\n",
    "cluster_dict = get_cluster_dict(link)\n",
    "print(cluster_dict)\n",
    "hrp2 = recClusterVar(cluster_dict,link, a.cov())\n",
    "hrp2.plot.bar()\n",
    "mpl.show()\n",
    "sortIx=getQuasiDiag(link) \n",
    "sortIx=a.cov().index[sortIx].tolist() # recover labels \n",
    "hrp=getRecBipart(a.cov(),sortIx)\n",
    "hrp.plot.bar()\n",
    "mpl.show()\n",
    "pd.DataFrame(risk_parity(a.cov())).plot.bar()\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.read_csv('divratios_stats_risk_parity_single.csv')['0'].mean())\n",
    "print(pd.read_csv('divratios_stats_max_div_port_single.csv')['0'].mean())\n",
    "print(pd.read_csv('divratios_stats_GMVPortfolio_single.csv')['0'].mean())\n",
    "print(pd.read_csv('divratios_stats_GMVLOPortfolio_single.csv')['0'].mean())\n",
    "print(pd.read_csv('divratios_stats_GETIVP_single.csv')['0'].mean())\n",
    "print(pd.read_csv('divratios_stats_GETHRP2_single.csv')['0'].mean())\n",
    "print(pd.read_csv('divratios_stats_GETHRP_single.csv')['0'].mean())\n",
    "print(pd.read_csv('divratios_stats_ewPortfolio_single.csv')['0'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_risk_deviance(rcs,mean=True):\n",
    "    deviances=[]\n",
    "    for i in rcs:\n",
    "        sam = 0\n",
    "        for j in i:\n",
    "            sam = sam+abs(j-1/len(i))\n",
    "        deviances.append(sam)\n",
    "    if mean:    \n",
    "        return np.array(deviances).mean()\n",
    "    else:\n",
    "        return np.array(deviances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in rcs:\n",
    "    rcs[i].to_csv('rcs {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in divratios:\n",
    "    divratios[i].to_csv('divratios {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
