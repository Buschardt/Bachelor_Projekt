{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as mpl\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import random, numpy as np, pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "1+2+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMVPortfolio(covMatrix):\n",
    "    return np.dot(np.linalg.inv(covMatrix),\n",
    "                  np.ones(len(covMatrix)))/np.dot(np.dot(np.transpose(np.ones(len(covMatrix))),\n",
    "                                                  np.linalg.inv(covMatrix)),np.ones(len(covMatrix)))\n",
    "\n",
    "\n",
    "def getIVP(cov,**kargs):\n",
    "    #Compute the inverse-variance portfolio\n",
    "    ivp=1./np.diag(cov)\n",
    "    ivp/=ivp.sum()\n",
    "    return ivp\n",
    "\n",
    "def getClusterVar(cov,cItems):\n",
    "    #Compute variance per cluster\n",
    "    cov_=cov.loc[cItems,cItems] # matrix slice\n",
    "    w_=getIVP(cov_).reshape(-1,1)\n",
    "    cVar=np.dot(np.dot(w_.T,cov_),w_)[0,0]\n",
    "    return cVar\n",
    "\n",
    "def getQuasiDiag(link):\n",
    "    # Sort clustered items by distance\n",
    "    link=link.astype(int)\n",
    "    sortIx=pd.Series([link[-1,0],link[-1,1]])\n",
    "    numItems=link[-1,3] #number of original items\n",
    "    while sortIx.max()>=numItems:\n",
    "        sortIx.index=range(0,sortIx.shape[0]*2,2) #make space\n",
    "        df0=sortIx[sortIx>=numItems] # find clusters\n",
    "        i = df0.index;j=df0.values-numItems\n",
    "        sortIx[i]=link[j,0] # item 1\n",
    "        df0=pd.Series(link[j,1],index=i+1)\n",
    "        sortIx=sortIx.append(df0) # item 2\n",
    "        sortIx=sortIx.sort_index() #re-sort\n",
    "        sortIx.index=range(sortIx.shape[0]) # re-index\n",
    "    return sortIx.tolist()\n",
    "\n",
    "def getRecBipart(cov,sortIx):\n",
    "    # Compute HRP alloc\n",
    "    w=pd.Series(1,index=sortIx)\n",
    "    cItems=[sortIx] # initialize all items in one cluster\n",
    "    while len(cItems)>0:\n",
    "        cItems=[i[j:k] for i in cItems for j,k in ((0,len(i)//2),(len(i)//2,len(i))) if len(i)>1] # bi-section\n",
    "        for i in range(0,len(cItems),2):\n",
    "            cItems0=cItems[i] # cluster 1\n",
    "            cItems1=cItems[i+1] # cluster 2\n",
    "            cVar0=getClusterVar(cov,cItems0)\n",
    "            cVar1=getClusterVar(cov,cItems1)\n",
    "            alpha=1-cVar0/(cVar0+cVar1)\n",
    "            w[cItems0]*=alpha # weight 1\n",
    "            w[cItems1]*=1-alpha # weight 2\n",
    "            \n",
    "    return w\n",
    "\n",
    "def correlDist(corr):\n",
    "    # A distance matrix based on correlation, where 0<=d[i,j]<=1\n",
    "    #This is a proper diastance metric\n",
    "    dist=((1-corr)/2.)**.5 # distance matrix\n",
    "    return dist\n",
    "\n",
    "\n",
    "def plotCorrMatrix(path,corr,labels=None):\n",
    "    #Heatmap of the correlation matrix\n",
    "    if labels is None: labels=[]\n",
    "    mpl.pcolor(corr)\n",
    "    mpl.colorbar()\n",
    "    mpl.yticks(np.arange(.5,corr.shape[0]+.5),labels)\n",
    "    mpl.xticks(np.arange(.5,corr.shape[0]+.5),labels)\n",
    "    mpl.savefig(path)\n",
    "    mpl.clf();mpl.close()  #reset pylab\n",
    "    return\n",
    "    \n",
    "def generateData(nObs,size0,size1,sigma1,x=np.empty((0,1))):\n",
    "    #Time series of correlated variables\n",
    "    #1)generating some uncorrelated data\n",
    "    np.random.seed(seed=12345);random.seed(12345)\n",
    "    if len(x)==0:\n",
    "        x=np.random.normal(0,1,size=(nObs,size0)) # each row is a variable\n",
    "    #2) creating correlation between the variables\n",
    "    cols=[random.randint(0,size0-1) for i in range(size1)]\n",
    "    q=np.random.normal(0,sigma1,size=(nObs,len(cols)))\n",
    "    y=x[:,cols]+q\n",
    "    x=np.append(x,y,axis=1)\n",
    "    x=pd.DataFrame(x,columns=range(1,x.shape[1]+1))\n",
    "    return x#,cols\n",
    "\n",
    "def generateAutocorrelatedData(nObs,correlation,size):\n",
    "    x=np.random.normal(0,1,size=(1,size))\n",
    "    for i in range(nObs-1):\n",
    "        x=np.append(x,correlation*x[i]+np.random.normal(0,1,size=(1,size)),axis=0)\n",
    "    return x\n",
    "\n",
    "def generateCauchyDistData(nObs,size):\n",
    "    x=np.random.standard_cauchy(size=(nObs,size))\n",
    "    return pd.DataFrame(x)\n",
    "\n",
    "def generateT_DistData(nObs,size,df):\n",
    "    x=np.random.standard_t(df,size=(nObs,size))\n",
    "    return pd.DataFrame(x)\n",
    "    \n",
    "\n",
    "def main():\n",
    "    #1) Generate correlated data\n",
    "    nObs, size0,size1,sigma1,correlation = 264,5,10,1,-1\n",
    "    x = generateAutocorrelatedData(nObs,correlation,size0)\n",
    "    x,cols=generateData(nObs,size0,size1,sigma1)\n",
    "    print(findCorrelatedCols([(j+1,size0+i) for i,j in enumerate(cols,1)],size0))\n",
    "    cov,corr=x.cov(),x.corr()\n",
    "    # 2) compute and plot correl matrix\n",
    "    #corr=pd.DataFrame(np.array([[1,0.7,0.2],[0.7,1,-0.2],[0.2,-0.2,1]]))\n",
    "    plotCorrMatrix('HRP3_corr0.png',corr,labels=corr.columns)\n",
    "    # 3) cluster\n",
    "    dist=correlDist(corr)\n",
    "    link=sch.linkage(dist,'single')\n",
    "    sortIx=getQuasiDiag(link)\n",
    "    sortIx=corr.index[sortIx].tolist() #recover labels\n",
    "    df0=corr.loc[sortIx,sortIx] #re-order\n",
    "    plotCorrMatrix('HRP3_corr1.png',df0,labels=df0.columns)\n",
    "    #4) Capital allocation\n",
    "    hrp=getRecBipart(cov,sortIx)\n",
    "    return hrp\n",
    "\n",
    "def findCorrelatedCols(colnbs,size0):\n",
    "    keys = list(set([i[0] for i in colnbs]))\n",
    "    for i in range(1,size0+1):\n",
    "        if i not in keys:\n",
    "            keys.append(i)     \n",
    "    keys.sort()\n",
    "    clusters={key: [key] for key in keys}\n",
    "    for i in colnbs:\n",
    "        clusters[i[0]].append(i[1])\n",
    "    return clusters\n",
    "\n",
    "def clusterWeights(clusters, hrp):\n",
    "    weights={key:None for key in clusters.keys()}\n",
    "    for i in weights:\n",
    "        weights[i] = sum([hrp.loc[j] for j in clusters[i]])\n",
    "    return list(weights.values())\n",
    "\n",
    "def testStability():\n",
    "    nObs, size0,size1,sigma1,recalc_time, samplesize,correlation = 528,5,5,0.5,22,264,0\n",
    "    #x = generateAutocorrelatedData(nObs,correlation,size0)\n",
    "    x,cols=generateData(nObs,size0,size1,sigma1)\n",
    "    clusters=findCorrelatedCols([(j+1,size0+i) for i,j in enumerate(cols,1)],size0)\n",
    "    clusterweights=[]\n",
    "    weights=[]\n",
    "    print(clusters)\n",
    "    for i in range(int((nObs-samplesize)/recalc_time)+1):\n",
    "        x_sample = x.iloc[i*recalc_time:samplesize+recalc_time*i]\n",
    "        cov,corr=x_sample.cov(),x_sample.corr()\n",
    "        dist=correlDist(corr)\n",
    "        link=sch.linkage(dist,'single')\n",
    "        sortIx=getQuasiDiag(link)\n",
    "        sortIx=corr.index[sortIx].tolist() #recover labels\n",
    "        df0=corr.loc[sortIx,sortIx] #re-order\n",
    "        hrp=getRecBipart(cov,sortIx)\n",
    "        plotCorrMatrix('HRP3_corr{}.png'.format(i),df0,labels=df0.columns)\n",
    "        clusterweights.append(clusterWeights(clusters,hrp))\n",
    "        weights.append(list(hrp.values))\n",
    "        print(hrp)\n",
    "    return pd.DataFrame(weights)\n",
    "    return pd.DataFrame(clusterweights)\n",
    "\n",
    "\n",
    "def calc_diversification_ratio(w, V):\n",
    "    print(w)\n",
    "    print(V)\n",
    "    # average weighted vol\n",
    "    w_vol = np.dot(np.sqrt(np.diag(V)), w.T)\n",
    "    # portfolio vol\n",
    "    port_vol = np.sqrt(calculate_portfolio_var(w, V))\n",
    "    diversification_ratio = w_vol/port_vol\n",
    "    # return negative for minimization problem (maximize = minimize -)\n",
    "    return -diversification_ratio\n",
    "def sumadw0(w0):\n",
    "    return sum(w0)-1\n",
    "\n",
    "def max_div_port(w0, V, bnd=None, long_only=True):\n",
    "    # w0: initial weight\n",
    "    # V: covariance matrix\n",
    "    # bnd: individual position limit\n",
    "    # long only: long only constraint\n",
    "    cons = ({'type': 'eq', 'fun': sumadw0},)\n",
    "    \n",
    "    if long_only: # add in long only constraint\n",
    "        cons = cons + ({'type': 'ineq', 'fun':  0},)\n",
    "    res = minimize(calc_diversification_ratio, w0, bounds=bnd, args=V, method='SLSQP')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_div_port(np.array([1/100]*10),generateData(100,5,5,0.1),bnd=[(0,None)]*10)\n",
    "#testStability().plot(figsize=(13,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewPortfolio(n):\n",
    "    return n*[1/n]\n",
    "\n",
    "def maximumDiversification(designMatrix):\n",
    "    return np.dot(np.linalg.inv(designMatrix.corr()),\n",
    "                  np.diag(corrMatrix))/np.dot(np.dot(np.transpose(np.diag(corrMatrix)),\n",
    "                                                    np.linalg.inv(corrMatrix)),np.diag(corrMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x,cols=generateData(1000,5,5,1)\n",
    "print(cols)\n",
    "print(pd.DataFrame(x).corr())\n",
    "print(maximumDiversification(pd.DataFrame(x).corr()))\n",
    "sum(maximumDiversification(pd.DataFrame(x).corr()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMVPortfolio(np.array([[0.08,0.05,-0.015],[0.05,0.15,0.000],[-0.015,0.000,0.25]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(np.transpose(np.ones(3)),np.linalg.inv(np.array([[0.08,0.05,-0.015],[0.05,0.15,0.000],[-0.015,0.000,0.25]])),np.ones(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(np.dot(np.ones(3),np.linalg.inv(np.array([[0.08,0.05,-0.015],[0.05,0.15,0.000],[-0.015,0.000,0.25]])))\n",
    "              ,np.transpose(np.ones(3)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
