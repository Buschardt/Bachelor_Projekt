{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mpl\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import random, numpy as np, pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "import scipy.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Equally Weighted Portfolio\n",
    "def ewPortfolio(n):\n",
    "    return n*[1/n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Risk Parity\n",
    "def calculate_risk_contribution(w,V):\n",
    "    # function that calculates asset contribution to total risk\n",
    "    w = np.matrix(w)\n",
    "    sigma = np.sqrt(calculate_portfolio_var(w,V))\n",
    "    # Marginal Risk Contribution\n",
    "    MRC = np.dot(V,w.T)\n",
    "    # Risk Contribution\n",
    "    RC = np.multiply(MRC,w.T)/sigma\n",
    "    return RC\n",
    "\n",
    "def risk_budget_objective(x,pars):\n",
    "    # calculate portfolio risk\n",
    "    V = pars[0]# covariance table\n",
    "    x_t = pars[1] # risk target in percent of portfolio risk\n",
    "    sig_p =  np.sqrt(calculate_portfolio_var(x,V)) # portfolio sigma\n",
    "    risk_target = np.asmatrix(np.multiply(sig_p,x_t))\n",
    "    asset_RC = calculate_risk_contribution(x,V)\n",
    "    J = sum(np.square(asset_RC-risk_target.T)) # sum of squared error\n",
    "    return J\n",
    "\n",
    "def risk_parity(riskbudget,w0,V):\n",
    "    x_t = riskbudget # your risk budget percent of total portfolio risk (equal risk)\n",
    "    cons = ({'type': 'eq', 'fun': total_weight_constraint},\n",
    "    {'type': 'ineq', 'fun': long_only_constraint})\n",
    "    res= minimize(risk_budget_objective, w0, args=[V,x_t], method='SLSQP',constraints=cons) \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global minimum variance\n",
    "def GMVPortfolio(covMatrix):\n",
    "    return np.dot(np.linalg.inv(covMatrix),\n",
    "                  np.ones(len(covMatrix)))/np.dot(np.dot(np.transpose(np.ones(len(covMatrix))),\n",
    "                                                  np.linalg.inv(covMatrix)),np.ones(len(covMatrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global minimum variance - long onlt\n",
    "def GMVLOPortfolio(covMatrix):\n",
    "    x0=[1/len(covMatrix)]*len(covMatrix)\n",
    "    cons = ({'type': 'eq', 'fun': total_weight_constraint},\n",
    "    {'type': 'ineq', 'fun': long_only_constraint})\n",
    "    res = minimize(calculate_portfolio_var,x0,args=covMatrix,method='SLSQP',constraints=cons)\n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse variance portfolio (Risk parity fra De Prado)\n",
    "def getIVP(cov,**kargs):\n",
    "    #Compute the inverse-variance portfolio\n",
    "    ivp=1./np.diag(cov)\n",
    "    ivp/=ivp.sum()\n",
    "    return ivp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximum diversification\n",
    "def calculate_portfolio_var(w,V):\n",
    "    # function that calculates portfolio risk\n",
    "    return (np.dot(np.dot(w,V),w.T))\n",
    "\n",
    "def calc_diversification_ratio(w, V):\n",
    "    # average weighted vol\n",
    "    w_vol = np.dot(np.sqrt(np.diag(V)), w.T)\n",
    "    # portfolio vol\n",
    "    port_vol = np.sqrt(calculate_portfolio_var(w, V))\n",
    "    diversification_ratio = w_vol/port_vol\n",
    "    # return negative for minimization problem (maximize = minimize -)\n",
    "    return -diversification_ratio\n",
    "\n",
    "def total_weight_constraint(x):\n",
    "    return np.sum(x)-1\n",
    "\n",
    "def long_only_constraint(x):\n",
    "    return x\n",
    "\n",
    "def max_div_port(w0, V, bnd=None, long_only=True):\n",
    "    # w0: initial weight\n",
    "    # V: covariance matrix\n",
    "    # bnd: individual position limit\n",
    "    # long only: long only constraint\n",
    "    cons = ({'type': 'eq', 'fun': total_weight_constraint},)\n",
    "    if long_only: # add in long only constraint\n",
    "        cons = cons + ({'type': 'ineq', 'fun':  long_only_constraint},)\n",
    "    res = minimize(calc_diversification_ratio, w0, bounds=bnd, args=V, method='SLSQP', constraints=cons)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HRP\n",
    "def getClusterVar(cov,cItems):\n",
    "    #Compute variance per cluster\n",
    "    cov_=cov.loc[cItems,cItems] # matrix slice\n",
    "    w_=getIVP(cov_).reshape(-1,1)\n",
    "    cVar=np.dot(np.dot(w_.T,cov_),w_)[0,0]\n",
    "    return cVar\n",
    "\n",
    "def getQuasiDiag(link):\n",
    "    # Sort clustered items by distance\n",
    "    link=link.astype(int)\n",
    "    sortIx=pd.Series([link[-1,0],link[-1,1]])\n",
    "    numItems=link[-1,3] #number of original items\n",
    "    while sortIx.max()>=numItems:\n",
    "        sortIx.index=range(0,sortIx.shape[0]*2,2) #make space\n",
    "        df0=sortIx[sortIx>=numItems] # find clusters\n",
    "        i = df0.index;j=df0.values-numItems\n",
    "        sortIx[i]=link[j,0] # item 1\n",
    "        df0=pd.Series(link[j,1],index=i+1)\n",
    "        sortIx=sortIx.append(df0) # item 2\n",
    "        sortIx=sortIx.sort_index() #re-sort\n",
    "        sortIx.index=range(sortIx.shape[0]) # re-index\n",
    "    return sortIx.tolist()\n",
    "\n",
    "def getRecBipart(cov,sortIx):\n",
    "    # Compute HRP alloc\n",
    "    w=pd.Series(1,index=sortIx)\n",
    "    cItems=[sortIx] # initialize all items in one cluster\n",
    "    while len(cItems)>0:\n",
    "        cItems=[i[j:k] for i in cItems for j,k in ((0,len(i)//2),(len(i)//2,len(i))) if len(i)>1] # bi-section\n",
    "        for i in range(0,len(cItems),2):\n",
    "            cItems0=cItems[i] # cluster 1\n",
    "            cItems1=cItems[i+1] # cluster 2\n",
    "            cVar0=getClusterVar(cov,cItems0)\n",
    "            cVar1=getClusterVar(cov,cItems1)\n",
    "            alpha=1-cVar0/(cVar0+cVar1)\n",
    "            w[cItems0]*=alpha # weight 1\n",
    "            w[cItems1]*=1-alpha # weight 2\n",
    "            \n",
    "    return w\n",
    "\n",
    "def correlDist(corr):\n",
    "    # A distance matrix based on correlation, where 0<=d[i,j]<=1\n",
    "    #This is a proper diastance metric\n",
    "    dist=((1-corr)/2.)**.5 # distance matrix\n",
    "    return dist\n",
    "\n",
    "\n",
    "def plotCorrMatrix(path,corr,labels=None):\n",
    "    #Heatmap of the correlation matrix\n",
    "    if labels is None: labels=[]\n",
    "    mpl.pcolor(corr)\n",
    "    mpl.colorbar()\n",
    "    mpl.yticks(np.arange(.5,corr.shape[0]+.5),labels)\n",
    "    mpl.xticks(np.arange(.5,corr.shape[0]+.5),labels)\n",
    "    mpl.savefig(path)\n",
    "    mpl.clf();mpl.close()  #reset pylab\n",
    "    return\n",
    "    \n",
    "def generateData(nObs,size0,size1,sigma1,x=np.empty((0,1))):\n",
    "    #Time series of correlated variables\n",
    "    #1)generating some uncorrelated data\n",
    "    np.random.seed(seed=12345);random.seed(12345)\n",
    "    if len(x)==0:\n",
    "        x=np.random.normal(0,1,size=(nObs,size0)) # each row is a variable\n",
    "    #2) creating correlation between the variables\n",
    "    cols=[random.randint(0,size0-1) for i in range(size1)]\n",
    "    q=np.random.normal(0,sigma1,size=(nObs,len(cols)))\n",
    "    y=x[:,cols]+q\n",
    "    x=np.append(x,y,axis=1)\n",
    "    x=pd.DataFrame(x,columns=range(1,x.shape[1]+1))\n",
    "    return x,cols\n",
    "\n",
    "def generateAutocorrelatedData(nObs,correlation,size):\n",
    "    x=np.random.normal(0,1,size=(1,size))\n",
    "    for i in range(nObs-1):\n",
    "        x=np.append(x,correlation*x[i]+np.random.normal(0,1,size=(1,size)),axis=0)\n",
    "    return x\n",
    "\n",
    "def generateCauchyDistData(nObs,size):\n",
    "    x=np.random.standard_cauchy(size=(nObs,size))\n",
    "    return pd.DataFrame(x)\n",
    "\n",
    "def generateT_DistData(nObs,size,df):\n",
    "    x=np.random.standard_t(df,size=(nObs,size))\n",
    "    return pd.DataFrame(x)\n",
    "    \n",
    "def findCorrelatedCols(colnbs,size0):\n",
    "    keys = list(set([i[0] for i in colnbs]))\n",
    "    for i in range(1,size0+1):\n",
    "        if i not in keys:\n",
    "            keys.append(i)     \n",
    "    keys.sort()\n",
    "    clusters={key: [key] for key in keys}\n",
    "    for i in colnbs:\n",
    "        clusters[i[0]].append(i[1])\n",
    "    return clusters\n",
    "\n",
    "def clusterWeights(clusters, hrp):\n",
    "    weights={key:None for key in clusters.keys()}\n",
    "    for i in weights:\n",
    "        weights[i] = sum([hrp.loc[j] for j in clusters[i]])\n",
    "    return list(weights.values())\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    #1) Generate correlated data\n",
    "    nObs, size0,size1,sigma1,correlation = 1000,5,10,0.5,-1\n",
    "    #x = generateAutocorrelatedData(nObs,correlation,size0)\n",
    "    x, cols=generateData(nObs,size0,size1,sigma1)\n",
    "    print(findCorrelatedCols([(j+1,size0+i) for i,j in enumerate(cols,1)],size0))\n",
    "    cov,corr=x.cov(),x.corr()\n",
    "    # 2) compute and plot correl matrix\n",
    "    #corr=pd.DataFrame(np.array([[1,0.7,0.2],[0.7,1,-0.2],[0.2,-0.2,1]]))\n",
    "    plotCorrMatrix('HRP3_corr0.png',corr,labels=corr.columns)\n",
    "    # 3) cluster\n",
    "    dist=correlDist(corr)\n",
    "    link=sch.linkage(dist,'single')\n",
    "    sortIx=getQuasiDiag(link)\n",
    "    sortIx=corr.index[sortIx].tolist() #recover labels\n",
    "    df0=corr.loc[sortIx,sortIx] #re-order\n",
    "    plotCorrMatrix('HRP3_corr1.png',df0,labels=df0.columns)\n",
    "    #4) Capital allocation\n",
    "    hrp=getRecBipart(cov,sortIx)\n",
    "    \n",
    "    plot_weights(hrp, cov, x)\n",
    "    print(sortIx)\n",
    "    scipy.cluster.hierarchy.dendrogram(link, labels=[i+1 for i in range(15)])\n",
    "    \n",
    "    return hrp\n",
    "\n",
    "\n",
    "def testStability():\n",
    "    nObs, size0,size1,sigma1,recalc_time, samplesize,correlation = 528,5,5,0.5,22,264,0\n",
    "    #x = generateAutocorrelatedData(nObs,correlation,size0)\n",
    "    x,cols=generateData(nObs,size0,size1,sigma1)\n",
    "    clusters=findCorrelatedCols([(j+1,size0+i) for i,j in enumerate(cols,1)],size0)\n",
    "    clusterweights=[]\n",
    "    weights=[]\n",
    "    print(clusters)\n",
    "    for i in range(int((nObs-samplesize)/recalc_time)+1):\n",
    "        x_sample = x.iloc[i*recalc_time:samplesize+recalc_time*i]\n",
    "        cov,corr=x_sample.cov(),x_sample.corr()\n",
    "        dist=correlDist(corr)\n",
    "        link=sch.linkage(dist,'complete')\n",
    "        sortIx=getQuasiDiag(link)\n",
    "        sortIx=corr.index[sortIx].tolist() #recover labels\n",
    "        df0=corr.loc[sortIx,sortIx] #re-order\n",
    "        hrp=getRecBipart(cov,sortIx)\n",
    "        plotCorrMatrix('HRP3_corr{}.png'.format(i),df0,labels=df0.columns)\n",
    "        clusterweights.append(clusterWeights(clusters,hrp))\n",
    "        weights.append(list(hrp.values))\n",
    "        print(hrp)\n",
    "    return pd.DataFrame(weights)\n",
    "    return pd.DataFrame(clusterweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_contribution(weights,cov):\n",
    "    portvar = np.dot(np.dot(weights,cov),weights.T)\n",
    "    rc=[(weights[i]*np.dot(cov,weights)[i])/portvar for i in range(len(weights))]\n",
    "    return rc\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Barplots of allocations\n",
    "def plot_weights(hrp, cov, data):\n",
    "    index = list(data.columns)\n",
    "    \n",
    "    #HRP\n",
    "    hrp = hrp.sort_values(ascending=False)\n",
    "    hrp.plot.bar(figsize = (15,7))\n",
    "    mpl.title(\"HRP\")\n",
    "    mpl.ylabel(\"Weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((0, 0.5))\n",
    "    mpl.show()\n",
    "\n",
    "    #Naive Risk-Parity\n",
    "    ivp = getIVP(cov)\n",
    "    ivp = pd.Series(ivp, index=index)\n",
    "    ivp = ivp.sort_values(ascending=False)\n",
    "    ivp.plot.bar(figsize = (15,7))\n",
    "    mpl.title(\"Naive Risk parity (IVP)\")\n",
    "    mpl.ylabel(\"Weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((0,0.5))\n",
    "    mpl.show()\n",
    "    \n",
    "    #Risk Parity\n",
    "    rp = risk_parity(np.array([1/len(cov)]*len(cov)),np.array([1/len(cov)]*len(cov)),cov).x\n",
    "    rp = pd.Series(rp, index=index)\n",
    "    rp = rp.sort_values(ascending=False)\n",
    "    rp.plot.bar(figsize = (15,7))\n",
    "    mpl.title(\"Risk Parity\")\n",
    "    mpl.ylabel(\"Weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((0, 0.5))\n",
    "    mpl.show()\n",
    "    \n",
    "    #GMV\n",
    "    gmv = GMVPortfolio(cov)\n",
    "    gmv = pd.Series(gmv, index=index)\n",
    "    gmv = gmv.sort_values(ascending=False)\n",
    "    gmv.plot.bar(figsize = (15,7))\n",
    "    mpl.title(\"GMV\")\n",
    "    mpl.ylabel(\"Weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((-0.1, 0.5))\n",
    "    mpl.show()\n",
    "    \n",
    "    #Maximum_Div_port\n",
    "    mdv = pd.Series(max_div_port(np.array([1/len(cov)]*len(cov)),cov).x, index=index)\n",
    "    mdv = mdv.sort_values(ascending=False)\n",
    "    mdv.plot.bar(figsize = (15,7))\n",
    "    mpl.title(\"Maximum Diversification Portfolio\")\n",
    "    mpl.ylabel(\"Weight\")\n",
    "    mpl.xlabel(\"Asset\")\n",
    "    mpl.ylim((-0.1, 0.5))\n",
    "    mpl.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data fra excel fil arp_strategies\n",
    "for i in range(5):\n",
    "    arp_data = pd.read_excel(\"arp_strategies.xlsx\", sheet_name=\"S_{}\".format(i+1))\n",
    "    arp_data = arp_data.iloc[2611:] #nogle strategier starter først fra 2005, så de første 2611 rækker fjernes\n",
    "    arp_data = arp_data.drop(['Date'], axis=1)\n",
    "    if i == 0:\n",
    "        arp_data_samlet = arp_data\n",
    "    else:\n",
    "        arp_data_samlet = pd.merge(arp_data_samlet,arp_data,right_index = True, left_index = True)\n",
    "\n",
    "arp_data = arp_data_samlet        \n",
    "arp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_2():\n",
    "    #1) data\n",
    "    x = arp_data\n",
    "    cov,corr=x.cov(),x.corr()\n",
    "    # 2) compute and plot correl matrix\n",
    "    #corr=pd.DataFrame(np.array([[1,0.7,0.2],[0.7,1,-0.2],[0.2,-0.2,1]]))\n",
    "    plotCorrMatrix('HRP3_corr0.png',corr,labels=corr.columns)\n",
    "    # 3) cluster\n",
    "    dist=correlDist(corr)\n",
    "    link=sch.linkage(dist,'median')\n",
    "    sortIx=getQuasiDiag(link)\n",
    "    sortIx=corr.index[sortIx].tolist() #recover labels\n",
    "    df0=corr.loc[sortIx,sortIx] #re-order\n",
    "    plotCorrMatrix('HRP3_corr1.png',df0,labels=df0.columns)\n",
    "    #4) Capital allocation\n",
    "    hrp=getRecBipart(cov,sortIx)\n",
    "    \n",
    "    plot_weights(hrp, cov, x)\n",
    "    mpl.figure(figsize=[20,10])\n",
    "    scipy.cluster.hierarchy.dendrogram(link)#, labels=sortIx)\n",
    "    \n",
    "    \n",
    "    return hrp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_3():\n",
    "    #Formålet er at se hvordan vægtene i porteføljen udvikler sig over tid.\n",
    "    re_calc_time,sample_size = 65.5,262\n",
    "    x = pd.DataFrame(arp_data)\n",
    "    \n",
    "    weights=[]\n",
    "    \n",
    "    for i in range(int((len(arp_data)-sample_size)/re_calc_time)):\n",
    "        x_sample = x.iloc[int(i*re_calc_time):int(i*re_calc_time)+sample_size]\n",
    "        cov,corr=x_sample.cov(),x_sample.corr()\n",
    "        dist=correlDist(corr)\n",
    "        link=sch.linkage(dist,'single')\n",
    "        sortIx=getQuasiDiag(link)\n",
    "        sortIx=corr.index[sortIx].tolist() #recover labels\n",
    "        hrp=getRecBipart(cov,sortIx)\n",
    "        weights.append(list(hrp.values))\n",
    "    return pd.DataFrame(weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = main_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weights.columns = arp_data.columns\n",
    "weight_stats=pd.DataFrame()\n",
    "weight_stats['mean'] = weights.mean()\n",
    "weight_stats['std.dev'] = weights.std()\n",
    "weight_stats['variance'] = weights.var()\n",
    "weight_stats.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.columns = arp_data.columns\n",
    "weight_stats=pd.DataFrame()\n",
    "weight_stats['mean'] = weights.mean()\n",
    "weight_stats['std.dev'] = weights.std()\n",
    "weight_stats['variance'] = weights.var()\n",
    "weight_stats.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
